<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Notes on Peixin Xu Peisonal Website</title>
    <link>https://xcanton.github.io/notes/</link>
    <description>Recent content in Notes on Peixin Xu Peisonal Website</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <atom:link href="https://xcanton.github.io/notes/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title></title>
      <link>https://xcanton.github.io/notes/llm_notes/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/readme/</guid>
      <description>_index </description>
    </item>
    <item>
      <title></title>
      <link>https://xcanton.github.io/notes/llm_notes/summary/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/summary/</guid>
      <description>Table of contents _index 🌌 before-llm-era _index Pre-trained Models _index Bert Transformer Attention _index Multi-head Attention Self-Attention 🌅 the-dawn-of-llm-era _index Bloom Other LLMs T5 Baichuan _index Baichuan-v2 ChatGLM _index ChatGLM-v2 ChatGLM-v3 GLM GPT Series _index gpt GPT2 Instruction-GPT GPT4 _index GPT-4 Architecture, Infrastructure,Training Dataset, Costs, Vision, MoE LLaMa Series _index LLaMa P5 _index How to Index Item IDs Qwen _index Qwen-v2 tuning _index DPO Lora NEFTune P-Tuning V2 P-Tuning PPO Q-Lora MOE-Lora 🤔 hullusion-and-training-innovations _index LM-Infinite knowledge-injection _index Can LMs Learn New Entities from Descriptions?</description>
    </item>
    <item>
      <title></title>
      <link>https://xcanton.github.io/notes/llm_notes/timeseries-prediction-of-llm/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/timeseries-prediction-of-llm/readme/</guid>
      <description>TimeSeries Prediction of LLM 目前大语言模型在时间序列预测的研究主要分成了两个方向:&#xA;一个是LLM -for-TS: 从零开始设计并预训练适用于时间序列的基础大模型, 然后可根据各种下游任务对模型进行微调。这条路径是最基本的解决方案，基于大量数据，通过预训练向模型灌输时间序列相关知识。但时间序列数据更专业且涉及隐私问题，获取大量的时间序列数据困难， 而且由于不同领域的时间序列数据存在重大差异，需要从头开始构建和训练针对不同垂域的各种模型； 一个是 TS-for-LLM: 设计相应机制对时间序列输入大模型进行适配，使其能够适用于现有的语言模型，从而基于现有的语言模型处理时间序列的各类任务。这条路径也具有一定的挑战性，需要超越原始语言模型的能力，补充时间序列语意信息。 两种方向的区别是：&#xA;一种比较自然的方法是将时间序列当成文本序列，但对处理多变量时间序列比较敏感； 第二种是对时间序列进行tokenize，设计一个模块编码TS tokens,并取代原有LLM的embedding layer， 其核心是创建LLM能够理解的ts embedding。 </description>
    </item>
    <item>
      <title></title>
      <link>https://xcanton.github.io/notes/llm_notes/timeseries-prediction-of-llm/ts-for-llm/large-language-models-are-zero-shot-time-series-forecasters-neurips2023/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/timeseries-prediction-of-llm/ts-for-llm/large-language-models-are-zero-shot-time-series-forecasters-neurips2023/</guid>
      <description>Large Language Models Are Zero-Shot Time Series Forecasters [NeurIPS2023] 论文地址：https://arxiv.org/abs/2310.07820 基本的时间预测，通过直接将时序数据用逗号分隔的方式转换成文本输入，通过模型生成其输出以逗号分隔的预测窗口数据。&#xA;文本生成模型是自回归模型，天然将文本token按位循环预测下一输出，与时序预测任务形式相同。 由于是研究Zero-Shot的效果，即在不训练的前提下进行预测，没有训练开销。 与深度模型不同的是，深度模型能够将数值读成一个整体，而文本生成模型由于Tokenizer的原因，对不同的模型和文本有不同的切分形式，所以该文章针对这一问题做出以下修正方案。 针对数字Token的改进 由于部分模型的Tokenizer对数字的切分方式不同，从而导致tokenize后对数字变化的表征不同。为了将数字按位划分，需要在黏着分词器（GPT-3）的输入文本中，将数字调整成空格分隔的形式（如果存在空格+数字的token）。而对非黏着分词器（LLaMa）这么做会导致模型认知不同，导致预测结果不好。&#xA;针对数值范围的改进 由于输入token数量的限制，序列如果本身数值大的会导致token数量变多，与自身变化无关，所以对数值范围进行标准化：&#xA;对LLaMa，论文采用sklearn.preprocessing.MinMaxScaler进行数值缩放（保留8位小数） 论文说GPT-3能够处理不同维度的数值，所以对GPT-3的数据采用的是仿射变换（affine transformation），具体公式如下 $$ (x_1, x_2, &amp;hellip;,x_T):x_t \Rightarrow (x_t - b)/a \ b=min(x_t) - \beta(max(x_i)-min(x_i)) $$&#xA;其中，beta是超参数，alpha是序列按位的百分位数。&#xA;针对随机性的改进 由于是zero-shot，部分模型并不能do_sample=False，导致结果受到随机性影响。所以论文在预测时多次预测（20次）取平均。&#xA;对文本生成的超参数（temperature scaling, logit bias, and nucleus sampling）做网格搜索，GPT搜索alpha、beta、temperature和precision，对LLaMa只搜索temperature，use α = 0.99, β = 0.3, precision = 3, nucleus = 0.9。&#xA;对生成概率改进，由于生成的文本 由于数据已经经过标准化，所以只考虑小数部分的连续概率。&#xA;结论 LLM可以找到数据的低复杂性解释，使他们能够zero-shot外推数值序列</description>
    </item>
    <item>
      <title></title>
      <link>https://xcanton.github.io/notes/llm_notes/timeseries-prediction-of-llm/ts-for-llm/llm-for-time-series-text-prototype-aligned-embedding-to-activate-llms-ability-for-time-series/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/timeseries-prediction-of-llm/ts-for-llm/llm-for-time-series-text-prototype-aligned-embedding-to-activate-llms-ability-for-time-series/</guid>
      <description>LLM for Time Series：Text Prototype Aligned Embedding to Activate LLM’s Ability for Time Series 论文地址：https://arxiv.org/abs/2308.08241 概要</description>
    </item>
    <item>
      <title></title>
      <link>https://xcanton.github.io/notes/llm_notes/timeseries-prediction-of-llm/ts-for-llm/llm4ts-two-stage-fine-tuning-for-time-series-forecasting-with-pre-trained-llms/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/timeseries-prediction-of-llm/ts-for-llm/llm4ts-two-stage-fine-tuning-for-time-series-forecasting-with-pre-trained-llms/</guid>
      <description>LLM4TS: Two-Stage Fine-Tuning for Time-Series Forecasting with Pre-Trained LLMs 论文地址：https://arxiv.org/abs/2308.08469 概要 输入处理类似上一篇文章， 主要尝试解决了两个主要问题： 如何将时间序列数据输入LLM？ 将时间序列转为patch，并通过1d-conv层将每个时序patch转为gpt2 的输入维度大小； 基于look-up 的patch location embedding; 通过channel 独立(通过权重共享间接实现cross-channel交互）的patching， 训练时采用instance-norm，预测时采用RevIN; 将每个patch内的第一个timestamp作为该patch的timestamp， patch内每个timestep的属性进行叠加作为该patch的属性； 如何与现有的LLM进行集成？ 时间序列进行自监督预训练(将LLM适配patch格式的时间序列数据）+下游预测任务微调； 自监督训练时，冻结llm的self-attention和FFN，重新训练输入端和layerNorm; 下游预测任务微调时，先进行linear probing(微调最后一层固定其他）,再微调所有参数的操作 ; 在多个公开数据集上的多变量时间序列预测，few-shot learing 和长时间序列预测超越专家网络模型。 </description>
    </item>
    <item>
      <title></title>
      <link>https://xcanton.github.io/notes/llm_notes/timeseries-prediction-of-llm/ts-for-llm/one-fits-all-power-general-time-series-analysis-by-pretrained-lm/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/timeseries-prediction-of-llm/ts-for-llm/one-fits-all-power-general-time-series-analysis-by-pretrained-lm/</guid>
      <description>One Fits All: Power General Time Series Analysis by Pretrained LM 论文地址：https://arxiv.org/abs/2302.11939 论文代码地址：https://github.com/DAMO-DI-ML/NeurIPS2023-One-Fits-All 概要 输入端先进行先对序列进行RevIN(instanceNorm), 缓解分布漂移； 再参考PatchTST， 将时间序列切割为片段处理，每个片段有postion embedding; 去除token embedding层，将切分的片段输入linear层转为模型需要的输入维度； 冻结编码知识的 multi-head 和FFN层，微调LayerNorm和位置编码； 文章认为基于文本域训练的llm适用于时间序列的一个可能原因是， 自监督的self-attention模块在训练过程中学会了和和具体数据无关的一些运算规则，比如PCA（通过对比对比两个模块的中间结果），使之成为一个广义的计算引擎； 以GPT2为backbone进行了时间序列异常检测、长短期预测等实验，效果均较好; </description>
    </item>
    <item>
      <title></title>
      <link>https://xcanton.github.io/notes/llm_notes/timeseries-prediction-of-llm/ts-for-llm/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/timeseries-prediction-of-llm/ts-for-llm/readme/</guid>
      <description>TS-for-LLM </description>
    </item>
    <item>
      <title></title>
      <link>https://xcanton.github.io/notes/llm_notes/timeseries-prediction-of-llm/ts-for-llm/tempo-prompt-based-generative-pre-trained-transformer-for-time-series-forecasting/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/timeseries-prediction-of-llm/ts-for-llm/tempo-prompt-based-generative-pre-trained-transformer-for-time-series-forecasting/</guid>
      <description>TEMPO: prompt-based generative pre-trained transformer for time series forecasting 论文地址：https://arxiv.org/abs/2310.04948 任务定义-序列预测任务 $$ {\hat x}^i_{t},{\hat x}^i_{t+1},\ldots,{\hat x}^i_{t+H-1}=F({x}^i_{t-K},{x}^i_{t-K+1},\ldots,{x}^i_{t-1};{V_i};&#x9;\Phi) $$&#xA;K是特征窗口长度，H是预测序列长度，通过过去K个序列值，预测下H个值 当需要预测多个特征时，i可以为不同的特征编号 $V_i$是对于特征V的prompt，$\phi$是模型参数 与传统文本生成任务不同，Tempo按照时序预测的任务形式构成输入prompt，通过先验规则进行特征工程。&#xA;加入序列先验知识 时序数据构成 $$ X_i=X^i_T + X^i_S+X^i_R $$&#xA;将原数据划分成长期特征、季节性特征和残差特征，一个输入数据由对应的三个特征值相加 长期特征（Trend） $$ X_T \in \mathbb{R}^{n \times L}=\frac {1} {2k+1} \sum^k_{j=-k}(X_{i+j}) $$&#xA;季节性特征（Seasonal，采用局部加权移动平均） $$ {Lowess\ Smoother}() $$&#xA;残差项 $$ X^i_R=X_i-X^i_T - X^i_S $$</description>
    </item>
  </channel>
</rss>
