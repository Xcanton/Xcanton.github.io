<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Peixin Xu Peisonal Website</title>
    <link>https://xcanton.github.io/</link>
    <description>Recent content on Peixin Xu Peisonal Website</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Wed, 01 Nov 2023 15:43:17 +0800</lastBuildDate>
    <atom:link href="https://xcanton.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>linking-writing-processes-to-writing-quality</title>
      <link>https://xcanton.github.io/kaggles/kaggle_notes/linking-writing-processes-to-writing-quality/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:17 +0800</pubDate>
      <guid>https://xcanton.github.io/kaggles/kaggle_notes/linking-writing-processes-to-writing-quality/</guid>
      <description> description: &amp;gt;- https://www.kaggle.com/competitions/linking-writing-processes-to-writing-quality Linking Writing Processes to Writing Quality </description>
    </item>
    <item>
      <title>baichuan-v2</title>
      <link>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/baichuan/baichuan-v2/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/baichuan/baichuan-v2/</guid>
      <description>Baichuan-v2 </description>
    </item>
    <item>
      <title>bert</title>
      <link>https://xcanton.github.io/notes/llm_notes/before-llm-era/pre-trained-models/bert/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/before-llm-era/pre-trained-models/bert/</guid>
      <description>Bert </description>
    </item>
    <item>
      <title>bloom</title>
      <link>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/bloom/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/bloom/</guid>
      <description>Bloom 论文BLOOM: A 176B-Parameter Open-Access Multilingual Language Model的阅读记录。</description>
    </item>
    <item>
      <title>can-lms-learn-new-entities-from-descriptions</title>
      <link>https://xcanton.github.io/notes/llm_notes/knowledge-injection/can-lms-learn-new-entities-from-descriptions/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/knowledge-injection/can-lms-learn-new-entities-from-descriptions/</guid>
      <description>Can LMs Learn New Entities from Descriptions? 论文Can LMs Learn New Entities from Descriptions? Challenges in Propagating Injected Knowledge的阅读记录。</description>
    </item>
    <item>
      <title>chatglm-v2</title>
      <link>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/chatglm/chatglm-v2/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/chatglm/chatglm-v2/</guid>
      <description>ChatGLM-v2 </description>
    </item>
    <item>
      <title>chatglm-v3</title>
      <link>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/chatglm/chatglm-v3/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/chatglm/chatglm-v3/</guid>
      <description>ChatGLM-v3 </description>
    </item>
    <item>
      <title>dpo</title>
      <link>https://xcanton.github.io/notes/llm_notes/tuning/dpo/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/tuning/dpo/</guid>
      <description>DPO </description>
    </item>
    <item>
      <title>flan</title>
      <link>https://xcanton.github.io/notes/llm_notes/other-papers/flan/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/other-papers/flan/</guid>
      <description>Flan 论文The Flan Collection: Designing Data and Methods for Effective Instruction Tuning的阅读记录。</description>
    </item>
    <item>
      <title>glm</title>
      <link>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/chatglm/glm/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/chatglm/glm/</guid>
      <description>GLM 论文GLM-130B: An Open Bilingual Pre-trained Model的阅读记录。</description>
    </item>
    <item>
      <title>gpt</title>
      <link>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/gpt-series/gpt/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/gpt-series/gpt/</guid>
      <description> description: &amp;lsquo;Paper :: Improving Language Understanding by Generative Pre-Training&amp;rsquo; GPT </description>
    </item>
    <item>
      <title>gpt-4-architecture-infrastructure-training-dataset-costs-vision-moe</title>
      <link>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/gpt-series/gpt4/gpt-4-architecture-infrastructure-training-dataset-costs-vision-moe/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/gpt-series/gpt4/gpt-4-architecture-infrastructure-training-dataset-costs-vision-moe/</guid>
      <description>GPT-4 Architecture, Infrastructure,Training Dataset, Costs, Vision, MoE </description>
    </item>
    <item>
      <title>gpt2</title>
      <link>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/gpt-series/gpt2/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/gpt-series/gpt2/</guid>
      <description>GPT2 </description>
    </item>
    <item>
      <title>how-to-index-item-ids</title>
      <link>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/p5/how-to-index-item-ids/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/p5/how-to-index-item-ids/</guid>
      <description>How to Index Item IDs 论文How to Index Item IDs for Recommendation Foundation Models的阅读记录。</description>
    </item>
    <item>
      <title>instruction-gpt</title>
      <link>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/gpt-series/instruction-gpt/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/gpt-series/instruction-gpt/</guid>
      <description>Instruction-GPT </description>
    </item>
    <item>
      <title>llama</title>
      <link>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/llama-series/llama/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/llama-series/llama/</guid>
      <description>LLaMa </description>
    </item>
    <item>
      <title>lm-infinite</title>
      <link>https://xcanton.github.io/notes/llm_notes/hullusion-and-training-innovations/lm-infinite/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/hullusion-and-training-innovations/lm-infinite/</guid>
      <description>LM-Infinite 论文《LM-INFINITE: SIMPLE ON-THE-FLY LENGTH GENERALIZATION FOR LARGE LANGUAGE MODELS的阅读记录。
Objective 针对开源大模型在长文本任务表现不佳的问题，提出对transformer的掩码与距离进行限制，提升模型在长文本生成内容的流畅度和相关性。
length generalization failure：生成长度超过训练文本长度（通常是预训练的平均长度）的回答。（the typical length in pre-training）</description>
    </item>
    <item>
      <title>lora</title>
      <link>https://xcanton.github.io/notes/llm_notes/tuning/lora/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/tuning/lora/</guid>
      <description>Lora </description>
    </item>
    <item>
      <title>multi-head-attention</title>
      <link>https://xcanton.github.io/notes/llm_notes/before-llm-era/pre-trained-models/attention/multi-head-attention/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/before-llm-era/pre-trained-models/attention/multi-head-attention/</guid>
      <description>Multi-head Attention </description>
    </item>
    <item>
      <title>neftune</title>
      <link>https://xcanton.github.io/notes/llm_notes/tuning/neftune/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/tuning/neftune/</guid>
      <description>NEFTune </description>
    </item>
    <item>
      <title>other-llms</title>
      <link>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/other-llms/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/other-llms/</guid>
      <description>Other LLMs </description>
    </item>
    <item>
      <title>p-tuning</title>
      <link>https://xcanton.github.io/notes/llm_notes/tuning/p-tuning/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/tuning/p-tuning/</guid>
      <description>P-Tuning </description>
    </item>
    <item>
      <title>p-tuning-v2</title>
      <link>https://xcanton.github.io/notes/llm_notes/tuning/p-tuning-v2/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/tuning/p-tuning-v2/</guid>
      <description>P-Tuning V2 </description>
    </item>
    <item>
      <title>ppo</title>
      <link>https://xcanton.github.io/notes/llm_notes/tuning/ppo/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/tuning/ppo/</guid>
      <description>PPO </description>
    </item>
    <item>
      <title>q-lora</title>
      <link>https://xcanton.github.io/notes/llm_notes/tuning/q-lora/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/tuning/q-lora/</guid>
      <description>Q-Lora </description>
    </item>
    <item>
      <title>qwen-v2</title>
      <link>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/qwen/qwen-v2/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/qwen/qwen-v2/</guid>
      <description>Qwen-v2 </description>
    </item>
    <item>
      <title>self-attention</title>
      <link>https://xcanton.github.io/notes/llm_notes/before-llm-era/pre-trained-models/attention/self-attention/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/before-llm-era/pre-trained-models/attention/self-attention/</guid>
      <description>Self-Attention </description>
    </item>
    <item>
      <title>t5</title>
      <link>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/t5/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/t5/</guid>
      <description>T5 论文Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer的阅读记录。</description>
    </item>
    <item>
      <title>transformer</title>
      <link>https://xcanton.github.io/notes/llm_notes/before-llm-era/pre-trained-models/transformer/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/before-llm-era/pre-trained-models/transformer/</guid>
      <description>Transformer </description>
    </item>
    <item>
      <title>tree-based-models-won-in-tabular</title>
      <link>https://xcanton.github.io/notes/llm_notes/other-papers/tabllm/tree-based-models-won-in-tabular/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/other-papers/tabllm/tree-based-models-won-in-tabular/</guid>
      <description>Tree-based models won in tabular 论文Why do tree-based models still outperform deep learning on tabular data?的阅读记录。</description>
    </item>
    <item>
      <title>where-to-go-next-for-recommendation-systems</title>
      <link>https://xcanton.github.io/notes/llm_notes/other-papers/where-to-go-next-for-recommendation-systems/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/other-papers/where-to-go-next-for-recommendation-systems/</guid>
      <description>Where to go Next for Recommendation Systems? 论文Where to Go Next for Recommender Systems? ID- vs. Modality-based Recommender Models Revisited的阅读记录。</description>
    </item>
    <item>
      <title></title>
      <link>https://xcanton.github.io/notes/llm_notes/before-llm-era/pre-trained-models/attention/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/before-llm-era/pre-trained-models/attention/readme/</guid>
      <description>Attention </description>
    </item>
    <item>
      <title></title>
      <link>https://xcanton.github.io/notes/llm_notes/before-llm-era/pre-trained-models/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/before-llm-era/pre-trained-models/readme/</guid>
      <description>Pre-trained Models </description>
    </item>
    <item>
      <title></title>
      <link>https://xcanton.github.io/notes/llm_notes/before-llm-era/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/before-llm-era/readme/</guid>
      <description>🌌 before-llm-era </description>
    </item>
    <item>
      <title></title>
      <link>https://xcanton.github.io/notes/llm_notes/hullusion-and-training-innovations/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/hullusion-and-training-innovations/readme/</guid>
      <description>🤔 hullusion-and-training-innovations </description>
    </item>
    <item>
      <title></title>
      <link>https://xcanton.github.io/notes/llm_notes/knowledge-injection/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/knowledge-injection/readme/</guid>
      <description>knowledge-injection </description>
    </item>
    <item>
      <title></title>
      <link>https://xcanton.github.io/notes/llm_notes/other-papers/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/other-papers/readme/</guid>
      <description>other-papers </description>
    </item>
    <item>
      <title></title>
      <link>https://xcanton.github.io/notes/llm_notes/other-papers/tabllm/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/other-papers/tabllm/readme/</guid>
      <description>TabLLM 论文TabLLM: Few-shot Classification of Tabular Data with Large Language Models的阅读记录。</description>
    </item>
    <item>
      <title></title>
      <link>https://xcanton.github.io/notes/llm_notes/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/readme/</guid>
      <description>_index </description>
    </item>
    <item>
      <title></title>
      <link>https://xcanton.github.io/notes/llm_notes/summary/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/summary/</guid>
      <description>Table of contents _index 🌌 before-llm-era _index Pre-trained Models _index Bert Transformer Attention _index Multi-head Attention Self-Attention 🌅 the-dawn-of-llm-era _index Bloom Other LLMs T5 Baichuan _index Baichuan-v2 ChatGLM _index ChatGLM-v2 ChatGLM-v3 GLM GPT Series _index gpt GPT2 Instruction-GPT GPT4 _index GPT-4 Architecture, Infrastructure,Training Dataset, Costs, Vision, MoE LLaMa Series _index LLaMa P5 _index How to Index Item IDs Qwen _index Qwen-v2 tuning _index DPO Lora NEFTune P-Tuning V2 P-Tuning PPO Q-Lora 🤔 hullusion-and-training-innovations _index LM-Infinite knowledge-injection _index Can LMs Learn New Entities from Descriptions?</description>
    </item>
    <item>
      <title></title>
      <link>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/baichuan/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/baichuan/readme/</guid>
      <description>Baichuan </description>
    </item>
    <item>
      <title></title>
      <link>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/chatglm/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/chatglm/readme/</guid>
      <description>ChatGLM 论文GLM: General Language Model Pretraining with Autoregressive Blank Infilling的阅读记录。</description>
    </item>
    <item>
      <title></title>
      <link>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/gpt-series/gpt4/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/gpt-series/gpt4/readme/</guid>
      <description>GPT4 </description>
    </item>
    <item>
      <title></title>
      <link>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/gpt-series/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/gpt-series/readme/</guid>
      <description>GPT Series </description>
    </item>
    <item>
      <title></title>
      <link>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/llama-series/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/llama-series/readme/</guid>
      <description>LLaMa Series </description>
    </item>
    <item>
      <title></title>
      <link>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/p5/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/p5/readme/</guid>
      <description>P5 论文Recommendation as Language Processing (RLP): A Unified Pretrain, Personalized Prompt &amp;amp; Predict Paradigm (P5)的阅读记录。</description>
    </item>
    <item>
      <title></title>
      <link>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/qwen/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/qwen/readme/</guid>
      <description>Qwen Qwen Technical Report的阅读记录。</description>
    </item>
    <item>
      <title></title>
      <link>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/readme/</guid>
      <description>🌅 the-dawn-of-llm-era </description>
    </item>
    <item>
      <title></title>
      <link>https://xcanton.github.io/notes/llm_notes/tuning/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/tuning/readme/</guid>
      <description>tuning </description>
    </item>
    <item>
      <title>Math Braintrainer</title>
      <link>https://xcanton.github.io/projects/math-brain-trainer/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/projects/math-brain-trainer/</guid>
      <description></description>
    </item>
    <item>
      <title>Surprise Surprise</title>
      <link>https://xcanton.github.io/articles/article/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/articles/article/</guid>
      <description>Thank you for your support! Hello. If you like this template, I&amp;rsquo;d be happy to get a coffee donation :)</description>
    </item>
  </channel>
</rss>
