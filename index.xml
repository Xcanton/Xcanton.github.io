<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Peixin Xu Peisonal Website</title>
    <link>https://xcanton.github.io/</link>
    <description>Recent content on Peixin Xu Peisonal Website</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Wed, 01 Nov 2023 15:43:17 +0800</lastBuildDate>
    <atom:link href="https://xcanton.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>linking-writing-processes-to-writing-quality</title>
      <link>https://xcanton.github.io/kaggles/kaggle_notes/readme/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:17 +0800</pubDate>
      <guid>https://xcanton.github.io/kaggles/kaggle_notes/readme/</guid>
      <description> description: &amp;gt;- https://www.kaggle.com/competitions/linking-writing-processes-to-writing-quality Linking Writing Processes to Writing Quality </description>
    </item>
    <item>
      <title>baichuan-v2</title>
      <link>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/baichuan/baichuan-v2/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/baichuan/baichuan-v2/</guid>
      <description>Baichuan-v2 </description>
    </item>
    <item>
      <title>bert</title>
      <link>https://xcanton.github.io/notes/llm_notes/before-llm-era/pre-trained-models/bert/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/before-llm-era/pre-trained-models/bert/</guid>
      <description>Bert </description>
    </item>
    <item>
      <title>bloom</title>
      <link>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/bloom/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/bloom/</guid>
      <description>Bloom è®ºæ–‡BLOOM: A 176B-Parameter Open-Access Multilingual Language Modelçš„é˜…è¯»è®°å½•ã€‚</description>
    </item>
    <item>
      <title>can-lms-learn-new-entities-from-descriptions</title>
      <link>https://xcanton.github.io/notes/llm_notes/knowledge-injection/can-lms-learn-new-entities-from-descriptions/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/knowledge-injection/can-lms-learn-new-entities-from-descriptions/</guid>
      <description>Can LMs Learn New Entities from Descriptions? è®ºæ–‡Can LMs Learn New Entities from Descriptions? Challenges in Propagating Injected Knowledgeçš„é˜…è¯»è®°å½•ã€‚</description>
    </item>
    <item>
      <title>chatglm-v2</title>
      <link>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/chatglm/chatglm-v2/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/chatglm/chatglm-v2/</guid>
      <description>ChatGLM-v2 </description>
    </item>
    <item>
      <title>chatglm-v3</title>
      <link>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/chatglm/chatglm-v3/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/chatglm/chatglm-v3/</guid>
      <description>ChatGLM-v3 </description>
    </item>
    <item>
      <title>dpo</title>
      <link>https://xcanton.github.io/notes/llm_notes/tuning/dpo/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/tuning/dpo/</guid>
      <description>DPO </description>
    </item>
    <item>
      <title>flan</title>
      <link>https://xcanton.github.io/notes/llm_notes/other-papers/flan/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/other-papers/flan/</guid>
      <description>Flan è®ºæ–‡The Flan Collection: Designing Data and Methods for Effective Instruction Tuningçš„é˜…è¯»è®°å½•ã€‚</description>
    </item>
    <item>
      <title>glm</title>
      <link>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/chatglm/glm/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/chatglm/glm/</guid>
      <description>GLM è®ºæ–‡GLM-130B: An Open Bilingual Pre-trained Modelçš„é˜…è¯»è®°å½•ã€‚</description>
    </item>
    <item>
      <title>gpt</title>
      <link>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/gpt-series/gpt/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/gpt-series/gpt/</guid>
      <description> description: &amp;lsquo;Paper :: Improving Language Understanding by Generative Pre-Training&amp;rsquo; GPT </description>
    </item>
    <item>
      <title>gpt-4-architecture-infrastructure-training-dataset-costs-vision-moe</title>
      <link>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/gpt-series/gpt4/gpt-4-architecture-infrastructure-training-dataset-costs-vision-moe/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/gpt-series/gpt4/gpt-4-architecture-infrastructure-training-dataset-costs-vision-moe/</guid>
      <description>GPT-4 Architecture, Infrastructure,Training Dataset, Costs, Vision, MoE </description>
    </item>
    <item>
      <title>gpt2</title>
      <link>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/gpt-series/gpt2/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/gpt-series/gpt2/</guid>
      <description>GPT2 </description>
    </item>
    <item>
      <title>how-to-index-item-ids</title>
      <link>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/p5/how-to-index-item-ids/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/p5/how-to-index-item-ids/</guid>
      <description>How to Index Item IDs è®ºæ–‡How to Index Item IDs for Recommendation Foundation Modelsçš„é˜…è¯»è®°å½•ã€‚</description>
    </item>
    <item>
      <title>instruction-gpt</title>
      <link>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/gpt-series/instruction-gpt/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/gpt-series/instruction-gpt/</guid>
      <description>Instruction-GPT </description>
    </item>
    <item>
      <title>llama</title>
      <link>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/llama-series/llama/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/llama-series/llama/</guid>
      <description>LLaMa </description>
    </item>
    <item>
      <title>lm-infinite</title>
      <link>https://xcanton.github.io/notes/llm_notes/hullusion-and-training-innovations/lm-infinite/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/hullusion-and-training-innovations/lm-infinite/</guid>
      <description>LM-Infinite è®ºæ–‡ã€ŠLM-INFINITE: SIMPLE ON-THE-FLY LENGTH GENERALIZATION FOR LARGE LANGUAGE MODELSçš„é˜…è¯»è®°å½•ã€‚
Objective é’ˆå¯¹å¼€æºå¤§æ¨¡å‹åœ¨é•¿æ–‡æœ¬ä»»åŠ¡è¡¨ç°ä¸ä½³çš„é—®é¢˜ï¼Œæå‡ºå¯¹transformerçš„æ©ç ä¸è·ç¦»è¿›è¡Œé™åˆ¶ï¼Œæå‡æ¨¡å‹åœ¨é•¿æ–‡æœ¬ç”Ÿæˆå†…å®¹çš„æµç•…åº¦å’Œç›¸å…³æ€§ã€‚
length generalization failureï¼šç”Ÿæˆé•¿åº¦è¶…è¿‡è®­ç»ƒæ–‡æœ¬é•¿åº¦ï¼ˆé€šå¸¸æ˜¯é¢„è®­ç»ƒçš„å¹³å‡é•¿åº¦ï¼‰çš„å›ç­”ã€‚ï¼ˆthe typical length in pre-trainingï¼‰</description>
    </item>
    <item>
      <title>lora</title>
      <link>https://xcanton.github.io/notes/llm_notes/tuning/lora/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/tuning/lora/</guid>
      <description>Lora </description>
    </item>
    <item>
      <title>multi-head-attention</title>
      <link>https://xcanton.github.io/notes/llm_notes/before-llm-era/pre-trained-models/attention/multi-head-attention/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/before-llm-era/pre-trained-models/attention/multi-head-attention/</guid>
      <description>Multi-head Attention </description>
    </item>
    <item>
      <title>neftune</title>
      <link>https://xcanton.github.io/notes/llm_notes/tuning/neftune/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/tuning/neftune/</guid>
      <description>NEFTune </description>
    </item>
    <item>
      <title>other-llms</title>
      <link>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/other-llms/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/other-llms/</guid>
      <description>Other LLMs </description>
    </item>
    <item>
      <title>p-tuning</title>
      <link>https://xcanton.github.io/notes/llm_notes/tuning/p-tuning/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/tuning/p-tuning/</guid>
      <description>P-Tuning </description>
    </item>
    <item>
      <title>p-tuning-v2</title>
      <link>https://xcanton.github.io/notes/llm_notes/tuning/p-tuning-v2/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/tuning/p-tuning-v2/</guid>
      <description>P-Tuning V2 </description>
    </item>
    <item>
      <title>ppo</title>
      <link>https://xcanton.github.io/notes/llm_notes/tuning/ppo/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/tuning/ppo/</guid>
      <description>PPO </description>
    </item>
    <item>
      <title>q-lora</title>
      <link>https://xcanton.github.io/notes/llm_notes/tuning/q-lora/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/tuning/q-lora/</guid>
      <description>Q-Lora </description>
    </item>
    <item>
      <title>qwen-v2</title>
      <link>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/qwen/qwen-v2/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/qwen/qwen-v2/</guid>
      <description>Qwen-v2 </description>
    </item>
    <item>
      <title>self-attention</title>
      <link>https://xcanton.github.io/notes/llm_notes/before-llm-era/pre-trained-models/attention/self-attention/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/before-llm-era/pre-trained-models/attention/self-attention/</guid>
      <description>Self-Attention </description>
    </item>
    <item>
      <title>t5</title>
      <link>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/t5/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/t5/</guid>
      <description>T5 è®ºæ–‡Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformerçš„é˜…è¯»è®°å½•ã€‚</description>
    </item>
    <item>
      <title>transformer</title>
      <link>https://xcanton.github.io/notes/llm_notes/before-llm-era/pre-trained-models/transformer/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/before-llm-era/pre-trained-models/transformer/</guid>
      <description>Transformer </description>
    </item>
    <item>
      <title>tree-based-models-won-in-tabular</title>
      <link>https://xcanton.github.io/notes/llm_notes/other-papers/tabllm/tree-based-models-won-in-tabular/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/other-papers/tabllm/tree-based-models-won-in-tabular/</guid>
      <description>Tree-based models won in tabular è®ºæ–‡Why do tree-based models still outperform deep learning on tabular data?çš„é˜…è¯»è®°å½•ã€‚</description>
    </item>
    <item>
      <title>where-to-go-next-for-recommendation-systems</title>
      <link>https://xcanton.github.io/notes/llm_notes/other-papers/where-to-go-next-for-recommendation-systems/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/other-papers/where-to-go-next-for-recommendation-systems/</guid>
      <description>Where to go Next for Recommendation Systems? è®ºæ–‡Where to Go Next for Recommender Systems? ID- vs. Modality-based Recommender Models Revisitedçš„é˜…è¯»è®°å½•ã€‚</description>
    </item>
    <item>
      <title></title>
      <link>https://xcanton.github.io/kaggles/kaggle_notes/enefit-predict-energy-behavior-of-prosumers/fang-an-tiao-yan/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/kaggles/kaggle_notes/enefit-predict-energy-behavior-of-prosumers/fang-an-tiao-yan/</guid>
      <description>æ–¹æ¡ˆè°ƒç ” æ•°æ®å¤„ç† è®­ç»ƒæ–¹å¼ æ¨¡å‹æ¶æ„ References </description>
    </item>
    <item>
      <title></title>
      <link>https://xcanton.github.io/kaggles/kaggle_notes/enefit-predict-energy-behavior-of-prosumers/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/kaggles/kaggle_notes/enefit-predict-energy-behavior-of-prosumers/readme/</guid>
      <description>Enefit - Predict Energy Behavior of Prosumers </description>
    </item>
    <item>
      <title></title>
      <link>https://xcanton.github.io/kaggles/kaggle_notes/summary/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/kaggles/kaggle_notes/summary/</guid>
      <description>Table of contents linking-writing-processes-to-writing-quality Enefit - Predict Energy Behavior of Prosumers æ–¹æ¡ˆè°ƒç ” </description>
    </item>
    <item>
      <title></title>
      <link>https://xcanton.github.io/notes/llm_notes/before-llm-era/pre-trained-models/attention/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/before-llm-era/pre-trained-models/attention/readme/</guid>
      <description>Attention </description>
    </item>
    <item>
      <title></title>
      <link>https://xcanton.github.io/notes/llm_notes/before-llm-era/pre-trained-models/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/before-llm-era/pre-trained-models/readme/</guid>
      <description>Pre-trained Models </description>
    </item>
    <item>
      <title></title>
      <link>https://xcanton.github.io/notes/llm_notes/before-llm-era/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/before-llm-era/readme/</guid>
      <description>ğŸŒŒ before-llm-era </description>
    </item>
    <item>
      <title></title>
      <link>https://xcanton.github.io/notes/llm_notes/hullusion-and-training-innovations/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/hullusion-and-training-innovations/readme/</guid>
      <description>ğŸ¤” hullusion-and-training-innovations </description>
    </item>
    <item>
      <title></title>
      <link>https://xcanton.github.io/notes/llm_notes/knowledge-injection/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/knowledge-injection/readme/</guid>
      <description>knowledge-injection </description>
    </item>
    <item>
      <title></title>
      <link>https://xcanton.github.io/notes/llm_notes/other-papers/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/other-papers/readme/</guid>
      <description>other-papers </description>
    </item>
    <item>
      <title></title>
      <link>https://xcanton.github.io/notes/llm_notes/other-papers/tabllm/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/other-papers/tabllm/readme/</guid>
      <description>TabLLM è®ºæ–‡TabLLM: Few-shot Classification of Tabular Data with Large Language Modelsçš„é˜…è¯»è®°å½•ã€‚</description>
    </item>
    <item>
      <title></title>
      <link>https://xcanton.github.io/notes/llm_notes/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/readme/</guid>
      <description>_index </description>
    </item>
    <item>
      <title></title>
      <link>https://xcanton.github.io/notes/llm_notes/summary/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/summary/</guid>
      <description>Table of contents _index ğŸŒŒ before-llm-era _index Pre-trained Models _index Bert Transformer Attention _index Multi-head Attention Self-Attention ğŸŒ… the-dawn-of-llm-era _index Bloom Other LLMs T5 Baichuan _index Baichuan-v2 ChatGLM _index ChatGLM-v2 ChatGLM-v3 GLM GPT Series _index gpt GPT2 Instruction-GPT GPT4 _index GPT-4 Architecture, Infrastructure,Training Dataset, Costs, Vision, MoE LLaMa Series _index LLaMa P5 _index How to Index Item IDs Qwen _index Qwen-v2 tuning _index DPO Lora NEFTune P-Tuning V2 P-Tuning PPO Q-Lora MOE-Lora ğŸ¤” hullusion-and-training-innovations _index LM-Infinite knowledge-injection _index Can LMs Learn New Entities from Descriptions?</description>
    </item>
    <item>
      <title></title>
      <link>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/baichuan/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/baichuan/readme/</guid>
      <description>Baichuan </description>
    </item>
    <item>
      <title></title>
      <link>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/chatglm/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/chatglm/readme/</guid>
      <description>ChatGLM è®ºæ–‡GLM: General Language Model Pretraining with Autoregressive Blank Infillingçš„é˜…è¯»è®°å½•ã€‚</description>
    </item>
    <item>
      <title></title>
      <link>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/gpt-series/gpt4/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/gpt-series/gpt4/readme/</guid>
      <description>GPT4 </description>
    </item>
    <item>
      <title></title>
      <link>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/gpt-series/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/gpt-series/readme/</guid>
      <description>GPT Series </description>
    </item>
    <item>
      <title></title>
      <link>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/llama-series/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/llama-series/readme/</guid>
      <description>LLaMa Series </description>
    </item>
    <item>
      <title></title>
      <link>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/p5/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/p5/readme/</guid>
      <description>P5 è®ºæ–‡Recommendation as Language Processing (RLP): A Unified Pretrain, Personalized Prompt &amp;amp; Predict Paradigm (P5)çš„é˜…è¯»è®°å½•ã€‚</description>
    </item>
    <item>
      <title></title>
      <link>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/qwen/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/qwen/readme/</guid>
      <description>Qwen Qwen Technical Reportçš„é˜…è¯»è®°å½•ã€‚</description>
    </item>
    <item>
      <title></title>
      <link>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/readme/</guid>
      <description>ğŸŒ… the-dawn-of-llm-era </description>
    </item>
    <item>
      <title></title>
      <link>https://xcanton.github.io/notes/llm_notes/timeseries-prediction-of-llm/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/timeseries-prediction-of-llm/readme/</guid>
      <description>TimeSeries Prediction of LLM ç›®å‰å¤§è¯­è¨€æ¨¡å‹åœ¨æ—¶é—´åºåˆ—é¢„æµ‹çš„ç ”ç©¶ä¸»è¦åˆ†æˆäº†ä¸¤ä¸ªæ–¹å‘:
ä¸€ä¸ªæ˜¯LLM -for-TS: ä»é›¶å¼€å§‹è®¾è®¡å¹¶é¢„è®­ç»ƒé€‚ç”¨äºæ—¶é—´åºåˆ—çš„åŸºç¡€å¤§æ¨¡å‹, ç„¶åå¯æ ¹æ®å„ç§ä¸‹æ¸¸ä»»åŠ¡å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚è¿™æ¡è·¯å¾„æ˜¯æœ€åŸºæœ¬çš„è§£å†³æ–¹æ¡ˆï¼ŒåŸºäºå¤§é‡æ•°æ®ï¼Œé€šè¿‡é¢„è®­ç»ƒå‘æ¨¡å‹çŒè¾“æ—¶é—´åºåˆ—ç›¸å…³çŸ¥è¯†ã€‚ä½†æ—¶é—´åºåˆ—æ•°æ®æ›´ä¸“ä¸šä¸”æ¶‰åŠéšç§é—®é¢˜ï¼Œè·å–å¤§é‡çš„æ—¶é—´åºåˆ—æ•°æ®å›°éš¾ï¼Œ è€Œä¸”ç”±äºä¸åŒé¢†åŸŸçš„æ—¶é—´åºåˆ—æ•°æ®å­˜åœ¨é‡å¤§å·®å¼‚ï¼Œéœ€è¦ä»å¤´å¼€å§‹æ„å»ºå’Œè®­ç»ƒé’ˆå¯¹ä¸åŒå‚åŸŸçš„å„ç§æ¨¡å‹ï¼› ä¸€ä¸ªæ˜¯ TS-for-LLM: è®¾è®¡ç›¸åº”æœºåˆ¶å¯¹æ—¶é—´åºåˆ—è¾“å…¥å¤§æ¨¡å‹è¿›è¡Œé€‚é…ï¼Œä½¿å…¶èƒ½å¤Ÿé€‚ç”¨äºç°æœ‰çš„è¯­è¨€æ¨¡å‹ï¼Œä»è€ŒåŸºäºç°æœ‰çš„è¯­è¨€æ¨¡å‹å¤„ç†æ—¶é—´åºåˆ—çš„å„ç±»ä»»åŠ¡ã€‚è¿™æ¡è·¯å¾„ä¹Ÿå…·æœ‰ä¸€å®šçš„æŒ‘æˆ˜æ€§ï¼Œéœ€è¦è¶…è¶ŠåŸå§‹è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œè¡¥å……æ—¶é—´åºåˆ—è¯­æ„ä¿¡æ¯ã€‚ ä¸¤ç§æ–¹å‘çš„åŒºåˆ«æ˜¯ï¼š
ä¸€ç§æ¯”è¾ƒè‡ªç„¶çš„æ–¹æ³•æ˜¯å°†æ—¶é—´åºåˆ—å½“æˆæ–‡æœ¬åºåˆ—ï¼Œä½†å¯¹å¤„ç†å¤šå˜é‡æ—¶é—´åºåˆ—æ¯”è¾ƒæ•æ„Ÿï¼› ç¬¬äºŒç§æ˜¯å¯¹æ—¶é—´åºåˆ—è¿›è¡Œtokenizeï¼Œè®¾è®¡ä¸€ä¸ªæ¨¡å—ç¼–ç TS tokens,å¹¶å–ä»£åŸæœ‰LLMçš„embedding layerï¼Œ å…¶æ ¸å¿ƒæ˜¯åˆ›å»ºLLMèƒ½å¤Ÿç†è§£çš„ts embeddingã€‚ </description>
    </item>
    <item>
      <title></title>
      <link>https://xcanton.github.io/notes/llm_notes/timeseries-prediction-of-llm/ts-for-llm/llm-for-time-series-text-prototype-aligned-embedding-to-activate-llms-ability-for-time-series/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/timeseries-prediction-of-llm/ts-for-llm/llm-for-time-series-text-prototype-aligned-embedding-to-activate-llms-ability-for-time-series/</guid>
      <description>LLM for Time Seriesï¼šText Prototype Aligned Embedding to Activate LLMâ€™s Ability for Time Series è®ºæ–‡åœ°å€ï¼šhttps://arxiv.org/abs/2308.08241 æ¦‚è¦</description>
    </item>
    <item>
      <title></title>
      <link>https://xcanton.github.io/notes/llm_notes/timeseries-prediction-of-llm/ts-for-llm/llm4ts-two-stage-fine-tuning-for-time-series-forecasting-with-pre-trained-llms/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/timeseries-prediction-of-llm/ts-for-llm/llm4ts-two-stage-fine-tuning-for-time-series-forecasting-with-pre-trained-llms/</guid>
      <description>LLM4TS: Two-Stage Fine-Tuning for Time-Series Forecasting with Pre-Trained LLMs è®ºæ–‡åœ°å€ï¼šhttps://arxiv.org/abs/2308.08469 æ¦‚è¦ è¾“å…¥å¤„ç†ç±»ä¼¼ä¸Šä¸€ç¯‡æ–‡ç« ï¼Œ ä¸»è¦å°è¯•è§£å†³äº†ä¸¤ä¸ªä¸»è¦é—®é¢˜ï¼š å¦‚ä½•å°†æ—¶é—´åºåˆ—æ•°æ®è¾“å…¥LLMï¼Ÿ å°†æ—¶é—´åºåˆ—è½¬ä¸ºpatchï¼Œå¹¶é€šè¿‡1d-convå±‚å°†æ¯ä¸ªæ—¶åºpatchè½¬ä¸ºgpt2 çš„è¾“å…¥ç»´åº¦å¤§å°ï¼› åŸºäºlook-up çš„patch location embedding; é€šè¿‡channel ç‹¬ç«‹(é€šè¿‡æƒé‡å…±äº«é—´æ¥å®ç°cross-channeläº¤äº’ï¼‰çš„patchingï¼Œ è®­ç»ƒæ—¶é‡‡ç”¨instance-normï¼Œé¢„æµ‹æ—¶é‡‡ç”¨RevIN; å°†æ¯ä¸ªpatchå†…çš„ç¬¬ä¸€ä¸ªtimestampä½œä¸ºè¯¥patchçš„timestampï¼Œ patchå†…æ¯ä¸ªtimestepçš„å±æ€§è¿›è¡Œå åŠ ä½œä¸ºè¯¥patchçš„å±æ€§ï¼› å¦‚ä½•ä¸ç°æœ‰çš„LLMè¿›è¡Œé›†æˆï¼Ÿ æ—¶é—´åºåˆ—è¿›è¡Œè‡ªç›‘ç£é¢„è®­ç»ƒ(å°†LLMé€‚é…patchæ ¼å¼çš„æ—¶é—´åºåˆ—æ•°æ®ï¼‰+ä¸‹æ¸¸é¢„æµ‹ä»»åŠ¡å¾®è°ƒï¼› è‡ªç›‘ç£è®­ç»ƒæ—¶ï¼Œå†»ç»“llmçš„self-attentionå’ŒFFNï¼Œé‡æ–°è®­ç»ƒè¾“å…¥ç«¯å’ŒlayerNorm; ä¸‹æ¸¸é¢„æµ‹ä»»åŠ¡å¾®è°ƒæ—¶ï¼Œå…ˆè¿›è¡Œlinear probing(å¾®è°ƒæœ€åä¸€å±‚å›ºå®šå…¶ä»–ï¼‰,å†å¾®è°ƒæ‰€æœ‰å‚æ•°çš„æ“ä½œ ; åœ¨å¤šä¸ªå…¬å¼€æ•°æ®é›†ä¸Šçš„å¤šå˜é‡æ—¶é—´åºåˆ—é¢„æµ‹ï¼Œfew-shot learing å’Œé•¿æ—¶é—´åºåˆ—é¢„æµ‹è¶…è¶Šä¸“å®¶ç½‘ç»œæ¨¡å‹ã€‚ </description>
    </item>
    <item>
      <title></title>
      <link>https://xcanton.github.io/notes/llm_notes/timeseries-prediction-of-llm/ts-for-llm/one-fits-all-power-general-time-series-analysis-by-pretrained-lm/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/timeseries-prediction-of-llm/ts-for-llm/one-fits-all-power-general-time-series-analysis-by-pretrained-lm/</guid>
      <description>One Fits All: Power General Time Series Analysis by Pretrained LM è®ºæ–‡åœ°å€ï¼šhttps://arxiv.org/abs/2302.11939 è®ºæ–‡ä»£ç åœ°å€ï¼šhttps://github.com/DAMO-DI-ML/NeurIPS2023-One-Fits-All æ¦‚è¦ è¾“å…¥ç«¯å…ˆè¿›è¡Œå…ˆå¯¹åºåˆ—è¿›è¡ŒRevIN(instanceNorm), ç¼“è§£åˆ†å¸ƒæ¼‚ç§»ï¼› å†å‚è€ƒPatchTSTï¼Œ å°†æ—¶é—´åºåˆ—åˆ‡å‰²ä¸ºç‰‡æ®µå¤„ç†ï¼Œæ¯ä¸ªç‰‡æ®µæœ‰postion embedding; å»é™¤token embeddingå±‚ï¼Œå°†åˆ‡åˆ†çš„ç‰‡æ®µè¾“å…¥linearå±‚è½¬ä¸ºæ¨¡å‹éœ€è¦çš„è¾“å…¥ç»´åº¦ï¼› å†»ç»“ç¼–ç çŸ¥è¯†çš„ multi-head å’ŒFFNå±‚ï¼Œå¾®è°ƒLayerNormå’Œä½ç½®ç¼–ç ï¼› æ–‡ç« è®¤ä¸ºåŸºäºæ–‡æœ¬åŸŸè®­ç»ƒçš„llmé€‚ç”¨äºæ—¶é—´åºåˆ—çš„ä¸€ä¸ªå¯èƒ½åŸå› æ˜¯ï¼Œ è‡ªç›‘ç£çš„self-attentionæ¨¡å—åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å­¦ä¼šäº†å’Œå’Œå…·ä½“æ•°æ®æ— å…³çš„ä¸€äº›è¿ç®—è§„åˆ™ï¼Œæ¯”å¦‚PCAï¼ˆé€šè¿‡å¯¹æ¯”å¯¹æ¯”ä¸¤ä¸ªæ¨¡å—çš„ä¸­é—´ç»“æœï¼‰ï¼Œä½¿ä¹‹æˆä¸ºä¸€ä¸ªå¹¿ä¹‰çš„è®¡ç®—å¼•æ“ï¼› ä»¥GPT2ä¸ºbackboneè¿›è¡Œäº†æ—¶é—´åºåˆ—å¼‚å¸¸æ£€æµ‹ã€é•¿çŸ­æœŸé¢„æµ‹ç­‰å®éªŒï¼Œæ•ˆæœå‡è¾ƒå¥½; </description>
    </item>
    <item>
      <title></title>
      <link>https://xcanton.github.io/notes/llm_notes/timeseries-prediction-of-llm/ts-for-llm/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/timeseries-prediction-of-llm/ts-for-llm/readme/</guid>
      <description>TS-for-LLM </description>
    </item>
    <item>
      <title></title>
      <link>https://xcanton.github.io/notes/llm_notes/timeseries-prediction-of-llm/ts-for-llm/tempo-prompt-based-generative-pre-trained-transformer-for-time-series-forecasting/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/timeseries-prediction-of-llm/ts-for-llm/tempo-prompt-based-generative-pre-trained-transformer-for-time-series-forecasting/</guid>
      <description>TEMPO: prompt-based generative pre-trained transformer for time series forecasting è®ºæ–‡åœ°å€ï¼šhttps://arxiv.org/abs/2310.04948 </description>
    </item>
    <item>
      <title></title>
      <link>https://xcanton.github.io/notes/llm_notes/tuning/moe-lora/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/tuning/moe-lora/</guid>
      <description>MOE-Lora </description>
    </item>
    <item>
      <title></title>
      <link>https://xcanton.github.io/notes/llm_notes/tuning/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/tuning/readme/</guid>
      <description>tuning </description>
    </item>
    <item>
      <title>Math Braintrainer</title>
      <link>https://xcanton.github.io/projects/math-brain-trainer/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/projects/math-brain-trainer/</guid>
      <description></description>
    </item>
    <item>
      <title>Surprise Surprise</title>
      <link>https://xcanton.github.io/articles/article/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/articles/article/</guid>
      <description>Thank you for your support! Hello. If you like this template, I&amp;rsquo;d be happy to get a coffee donation :)</description>
    </item>
  </channel>
</rss>
