<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Peixin Xu Peisonal Website</title>
    <link>https://xcanton.github.io/</link>
    <description>Recent content on Peixin Xu Peisonal Website</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Wed, 01 Nov 2023 15:43:17 +0800</lastBuildDate>
    <atom:link href="https://xcanton.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>linking-writing-processes-to-writing-quality</title>
      <link>https://xcanton.github.io/kaggles/kaggle_notes/linking-writing-processes-to-writing-quality/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:17 +0800</pubDate>
      <guid>https://xcanton.github.io/kaggles/kaggle_notes/linking-writing-processes-to-writing-quality/</guid>
      <description> description: &amp;gt;- https://www.kaggle.com/competitions/linking-writing-processes-to-writing-quality Linking Writing Processes to Writing Quality </description>
    </item>
    <item>
      <title>baichuan-v2</title>
      <link>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/baichuan/baichuan-v2/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/baichuan/baichuan-v2/</guid>
      <description>Baichuan-v2 </description>
    </item>
    <item>
      <title>bert</title>
      <link>https://xcanton.github.io/notes/llm_notes/before-llm-era/pre-trained-models/bert/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/before-llm-era/pre-trained-models/bert/</guid>
      <description>Bert </description>
    </item>
    <item>
      <title>bloom</title>
      <link>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/bloom/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/bloom/</guid>
      <description>Bloom è®ºæ–‡BLOOM: A 176B-Parameter Open-Access Multilingual Language Modelçš„é˜…è¯»è®°å½•ã€‚</description>
    </item>
    <item>
      <title>can-lms-learn-new-entities-from-descriptions</title>
      <link>https://xcanton.github.io/notes/llm_notes/knowledge-injection/can-lms-learn-new-entities-from-descriptions/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/knowledge-injection/can-lms-learn-new-entities-from-descriptions/</guid>
      <description>Can LMs Learn New Entities from Descriptions? è®ºæ–‡Can LMs Learn New Entities from Descriptions? Challenges in Propagating Injected Knowledgeçš„é˜…è¯»è®°å½•ã€‚</description>
    </item>
    <item>
      <title>chatglm-v2</title>
      <link>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/chatglm/chatglm-v2/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/chatglm/chatglm-v2/</guid>
      <description>ChatGLM-v2 </description>
    </item>
    <item>
      <title>chatglm-v3</title>
      <link>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/chatglm/chatglm-v3/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/chatglm/chatglm-v3/</guid>
      <description>ChatGLM-v3 </description>
    </item>
    <item>
      <title>dpo</title>
      <link>https://xcanton.github.io/notes/llm_notes/tuning/dpo/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/tuning/dpo/</guid>
      <description>DPO </description>
    </item>
    <item>
      <title>flan</title>
      <link>https://xcanton.github.io/notes/llm_notes/other-papers/flan/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/other-papers/flan/</guid>
      <description>Flan è®ºæ–‡The Flan Collection: Designing Data and Methods for Effective Instruction Tuningçš„é˜…è¯»è®°å½•ã€‚</description>
    </item>
    <item>
      <title>glm</title>
      <link>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/chatglm/glm/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/chatglm/glm/</guid>
      <description>GLM è®ºæ–‡GLM-130B: An Open Bilingual Pre-trained Modelçš„é˜…è¯»è®°å½•ã€‚</description>
    </item>
    <item>
      <title>gpt</title>
      <link>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/gpt-series/gpt/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/gpt-series/gpt/</guid>
      <description> description: &amp;lsquo;Paper :: Improving Language Understanding by Generative Pre-Training&amp;rsquo; GPT </description>
    </item>
    <item>
      <title>gpt-4-architecture-infrastructure-training-dataset-costs-vision-moe</title>
      <link>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/gpt-series/gpt4/gpt-4-architecture-infrastructure-training-dataset-costs-vision-moe/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/gpt-series/gpt4/gpt-4-architecture-infrastructure-training-dataset-costs-vision-moe/</guid>
      <description>GPT-4 Architecture, Infrastructure,Training Dataset, Costs, Vision, MoE </description>
    </item>
    <item>
      <title>gpt2</title>
      <link>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/gpt-series/gpt2/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/gpt-series/gpt2/</guid>
      <description>GPT2 </description>
    </item>
    <item>
      <title>how-to-index-item-ids</title>
      <link>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/p5/how-to-index-item-ids/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/p5/how-to-index-item-ids/</guid>
      <description>How to Index Item IDs è®ºæ–‡How to Index Item IDs for Recommendation Foundation Modelsçš„é˜…è¯»è®°å½•ã€‚</description>
    </item>
    <item>
      <title>instruction-gpt</title>
      <link>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/gpt-series/instruction-gpt/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/gpt-series/instruction-gpt/</guid>
      <description>Instruction-GPT </description>
    </item>
    <item>
      <title>llama</title>
      <link>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/llama-series/llama/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/llama-series/llama/</guid>
      <description>LLaMa </description>
    </item>
    <item>
      <title>lm-infinite</title>
      <link>https://xcanton.github.io/notes/llm_notes/hullusion-and-training-innovations/lm-infinite/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/hullusion-and-training-innovations/lm-infinite/</guid>
      <description>LM-Infinite è®ºæ–‡ã€ŠLM-INFINITE: SIMPLE ON-THE-FLY LENGTH GENERALIZATION FOR LARGE LANGUAGE MODELSçš„é˜…è¯»è®°å½•ã€‚
Objective é’ˆå¯¹å¼€æºå¤§æ¨¡å‹åœ¨é•¿æ–‡æœ¬ä»»åŠ¡è¡¨ç°ä¸ä½³çš„é—®é¢˜ï¼Œæå‡ºå¯¹transformerçš„æ©ç ä¸è·ç¦»è¿›è¡Œé™åˆ¶ï¼Œæå‡æ¨¡å‹åœ¨é•¿æ–‡æœ¬ç”Ÿæˆå†…å®¹çš„æµç•…åº¦å’Œç›¸å…³æ€§ã€‚
length generalization failureï¼šç”Ÿæˆé•¿åº¦è¶…è¿‡è®­ç»ƒæ–‡æœ¬é•¿åº¦ï¼ˆé€šå¸¸æ˜¯é¢„è®­ç»ƒçš„å¹³å‡é•¿åº¦ï¼‰çš„å›ç­”ã€‚ï¼ˆthe typical length in pre-trainingï¼‰</description>
    </item>
    <item>
      <title>lora</title>
      <link>https://xcanton.github.io/notes/llm_notes/tuning/lora/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/tuning/lora/</guid>
      <description>Lora </description>
    </item>
    <item>
      <title>multi-head-attention</title>
      <link>https://xcanton.github.io/notes/llm_notes/before-llm-era/pre-trained-models/attention/multi-head-attention/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/before-llm-era/pre-trained-models/attention/multi-head-attention/</guid>
      <description>Multi-head Attention </description>
    </item>
    <item>
      <title>neftune</title>
      <link>https://xcanton.github.io/notes/llm_notes/tuning/neftune/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/tuning/neftune/</guid>
      <description>NEFTune </description>
    </item>
    <item>
      <title>other-llms</title>
      <link>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/other-llms/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/other-llms/</guid>
      <description>Other LLMs </description>
    </item>
    <item>
      <title>p-tuning</title>
      <link>https://xcanton.github.io/notes/llm_notes/tuning/p-tuning/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/tuning/p-tuning/</guid>
      <description>P-Tuning </description>
    </item>
    <item>
      <title>p-tuning-v2</title>
      <link>https://xcanton.github.io/notes/llm_notes/tuning/p-tuning-v2/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/tuning/p-tuning-v2/</guid>
      <description>P-Tuning V2 </description>
    </item>
    <item>
      <title>ppo</title>
      <link>https://xcanton.github.io/notes/llm_notes/tuning/ppo/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/tuning/ppo/</guid>
      <description>PPO </description>
    </item>
    <item>
      <title>q-lora</title>
      <link>https://xcanton.github.io/notes/llm_notes/tuning/q-lora/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/tuning/q-lora/</guid>
      <description>Q-Lora </description>
    </item>
    <item>
      <title>qwen-v2</title>
      <link>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/qwen/qwen-v2/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/qwen/qwen-v2/</guid>
      <description>Qwen-v2 </description>
    </item>
    <item>
      <title>self-attention</title>
      <link>https://xcanton.github.io/notes/llm_notes/before-llm-era/pre-trained-models/attention/self-attention/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/before-llm-era/pre-trained-models/attention/self-attention/</guid>
      <description>Self-Attention </description>
    </item>
    <item>
      <title>t5</title>
      <link>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/t5/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/t5/</guid>
      <description>T5 è®ºæ–‡Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformerçš„é˜…è¯»è®°å½•ã€‚</description>
    </item>
    <item>
      <title>transformer</title>
      <link>https://xcanton.github.io/notes/llm_notes/before-llm-era/pre-trained-models/transformer/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/before-llm-era/pre-trained-models/transformer/</guid>
      <description>Transformer </description>
    </item>
    <item>
      <title>tree-based-models-won-in-tabular</title>
      <link>https://xcanton.github.io/notes/llm_notes/other-papers/tabllm/tree-based-models-won-in-tabular/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/other-papers/tabllm/tree-based-models-won-in-tabular/</guid>
      <description>Tree-based models won in tabular è®ºæ–‡Why do tree-based models still outperform deep learning on tabular data?çš„é˜…è¯»è®°å½•ã€‚</description>
    </item>
    <item>
      <title>where-to-go-next-for-recommendation-systems</title>
      <link>https://xcanton.github.io/notes/llm_notes/other-papers/where-to-go-next-for-recommendation-systems/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/other-papers/where-to-go-next-for-recommendation-systems/</guid>
      <description>Where to go Next for Recommendation Systems? è®ºæ–‡Where to Go Next for Recommender Systems? ID- vs. Modality-based Recommender Models Revisitedçš„é˜…è¯»è®°å½•ã€‚</description>
    </item>
    <item>
      <title></title>
      <link>https://xcanton.github.io/notes/llm_notes/before-llm-era/pre-trained-models/attention/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/before-llm-era/pre-trained-models/attention/readme/</guid>
      <description>Attention </description>
    </item>
    <item>
      <title></title>
      <link>https://xcanton.github.io/notes/llm_notes/before-llm-era/pre-trained-models/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/before-llm-era/pre-trained-models/readme/</guid>
      <description>Pre-trained Models </description>
    </item>
    <item>
      <title></title>
      <link>https://xcanton.github.io/notes/llm_notes/before-llm-era/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/before-llm-era/readme/</guid>
      <description>ğŸŒŒ before-llm-era </description>
    </item>
    <item>
      <title></title>
      <link>https://xcanton.github.io/notes/llm_notes/hullusion-and-training-innovations/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/hullusion-and-training-innovations/readme/</guid>
      <description>ğŸ¤” hullusion-and-training-innovations </description>
    </item>
    <item>
      <title></title>
      <link>https://xcanton.github.io/notes/llm_notes/knowledge-injection/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/knowledge-injection/readme/</guid>
      <description>knowledge-injection </description>
    </item>
    <item>
      <title></title>
      <link>https://xcanton.github.io/notes/llm_notes/other-papers/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/other-papers/readme/</guid>
      <description>other-papers </description>
    </item>
    <item>
      <title></title>
      <link>https://xcanton.github.io/notes/llm_notes/other-papers/tabllm/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/other-papers/tabllm/readme/</guid>
      <description>TabLLM è®ºæ–‡TabLLM: Few-shot Classification of Tabular Data with Large Language Modelsçš„é˜…è¯»è®°å½•ã€‚</description>
    </item>
    <item>
      <title></title>
      <link>https://xcanton.github.io/notes/llm_notes/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/readme/</guid>
      <description>_index </description>
    </item>
    <item>
      <title></title>
      <link>https://xcanton.github.io/notes/llm_notes/summary/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/summary/</guid>
      <description>Table of contents _index ğŸŒŒ before-llm-era _index Pre-trained Models _index Bert Transformer Attention _index Multi-head Attention Self-Attention ğŸŒ… the-dawn-of-llm-era _index Bloom Other LLMs T5 Baichuan _index Baichuan-v2 ChatGLM _index ChatGLM-v2 ChatGLM-v3 GLM GPT Series _index gpt GPT2 Instruction-GPT GPT4 _index GPT-4 Architecture, Infrastructure,Training Dataset, Costs, Vision, MoE LLaMa Series _index LLaMa P5 _index How to Index Item IDs Qwen _index Qwen-v2 tuning _index DPO Lora NEFTune P-Tuning V2 P-Tuning PPO Q-Lora ğŸ¤” hullusion-and-training-innovations _index LM-Infinite knowledge-injection _index Can LMs Learn New Entities from Descriptions?</description>
    </item>
    <item>
      <title></title>
      <link>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/baichuan/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/baichuan/readme/</guid>
      <description>Baichuan </description>
    </item>
    <item>
      <title></title>
      <link>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/chatglm/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/chatglm/readme/</guid>
      <description>ChatGLM è®ºæ–‡GLM: General Language Model Pretraining with Autoregressive Blank Infillingçš„é˜…è¯»è®°å½•ã€‚</description>
    </item>
    <item>
      <title></title>
      <link>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/gpt-series/gpt4/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/gpt-series/gpt4/readme/</guid>
      <description>GPT4 </description>
    </item>
    <item>
      <title></title>
      <link>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/gpt-series/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/gpt-series/readme/</guid>
      <description>GPT Series </description>
    </item>
    <item>
      <title></title>
      <link>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/llama-series/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/llama-series/readme/</guid>
      <description>LLaMa Series </description>
    </item>
    <item>
      <title></title>
      <link>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/p5/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/p5/readme/</guid>
      <description>P5 è®ºæ–‡Recommendation as Language Processing (RLP): A Unified Pretrain, Personalized Prompt &amp;amp; Predict Paradigm (P5)çš„é˜…è¯»è®°å½•ã€‚</description>
    </item>
    <item>
      <title></title>
      <link>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/qwen/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/qwen/readme/</guid>
      <description>Qwen Qwen Technical Reportçš„é˜…è¯»è®°å½•ã€‚</description>
    </item>
    <item>
      <title></title>
      <link>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/readme/</guid>
      <description>ğŸŒ… the-dawn-of-llm-era </description>
    </item>
    <item>
      <title></title>
      <link>https://xcanton.github.io/notes/llm_notes/tuning/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/tuning/readme/</guid>
      <description>tuning </description>
    </item>
    <item>
      <title>Math Braintrainer</title>
      <link>https://xcanton.github.io/projects/math-brain-trainer/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/projects/math-brain-trainer/</guid>
      <description></description>
    </item>
    <item>
      <title>Surprise Surprise</title>
      <link>https://xcanton.github.io/articles/article/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/articles/article/</guid>
      <description>Thank you for your support! Hello. If you like this template, I&amp;rsquo;d be happy to get a coffee donation :)</description>
    </item>
  </channel>
</rss>
