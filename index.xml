<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Peixin Xu Peisonal Website</title>
    <link>https://xcanton.github.io/</link>
    <description>Recent content on Peixin Xu Peisonal Website</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Wed, 01 Nov 2023 15:43:17 +0800</lastBuildDate>
    <atom:link href="https://xcanton.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>linking-writing-processes-to-writing-quality</title>
      <link>https://xcanton.github.io/kaggles/kaggle_notes/readme/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:17 +0800</pubDate>
      <guid>https://xcanton.github.io/kaggles/kaggle_notes/readme/</guid>
      <description> description: &amp;gt;- https://www.kaggle.com/competitions/linking-writing-processes-to-writing-quality Linking Writing Processes to Writing Quality </description>
    </item>
    <item>
      <title>baichuan-v2</title>
      <link>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/baichuan/baichuan-v2/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/baichuan/baichuan-v2/</guid>
      <description>Baichuan-v2 </description>
    </item>
    <item>
      <title>bert</title>
      <link>https://xcanton.github.io/notes/llm_notes/before-llm-era/pre-trained-models/bert/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/before-llm-era/pre-trained-models/bert/</guid>
      <description>Bert </description>
    </item>
    <item>
      <title>bloom</title>
      <link>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/bloom/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/bloom/</guid>
      <description>Bloom 论文BLOOM: A 176B-Parameter Open-Access Multilingual Language Model的阅读记录。</description>
    </item>
    <item>
      <title>can-lms-learn-new-entities-from-descriptions</title>
      <link>https://xcanton.github.io/notes/llm_notes/knowledge-injection/can-lms-learn-new-entities-from-descriptions/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/knowledge-injection/can-lms-learn-new-entities-from-descriptions/</guid>
      <description>Can LMs Learn New Entities from Descriptions? 论文Can LMs Learn New Entities from Descriptions? Challenges in Propagating Injected Knowledge的阅读记录。</description>
    </item>
    <item>
      <title>chatglm-v2</title>
      <link>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/chatglm/chatglm-v2/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/chatglm/chatglm-v2/</guid>
      <description>ChatGLM-v2 </description>
    </item>
    <item>
      <title>chatglm-v3</title>
      <link>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/chatglm/chatglm-v3/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/chatglm/chatglm-v3/</guid>
      <description>ChatGLM-v3 </description>
    </item>
    <item>
      <title>dpo</title>
      <link>https://xcanton.github.io/notes/llm_notes/tuning/dpo/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/tuning/dpo/</guid>
      <description>DPO </description>
    </item>
    <item>
      <title>flan</title>
      <link>https://xcanton.github.io/notes/llm_notes/other-papers/flan/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/other-papers/flan/</guid>
      <description>Flan 论文The Flan Collection: Designing Data and Methods for Effective Instruction Tuning的阅读记录。</description>
    </item>
    <item>
      <title>glm</title>
      <link>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/chatglm/glm/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/chatglm/glm/</guid>
      <description>GLM 论文GLM-130B: An Open Bilingual Pre-trained Model的阅读记录。</description>
    </item>
    <item>
      <title>gpt</title>
      <link>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/gpt-series/gpt/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/gpt-series/gpt/</guid>
      <description> description: &amp;lsquo;Paper :: Improving Language Understanding by Generative Pre-Training&amp;rsquo; GPT </description>
    </item>
    <item>
      <title>gpt-4-architecture-infrastructure-training-dataset-costs-vision-moe</title>
      <link>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/gpt-series/gpt4/gpt-4-architecture-infrastructure-training-dataset-costs-vision-moe/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/gpt-series/gpt4/gpt-4-architecture-infrastructure-training-dataset-costs-vision-moe/</guid>
      <description>GPT-4 Architecture, Infrastructure,Training Dataset, Costs, Vision, MoE </description>
    </item>
    <item>
      <title>gpt2</title>
      <link>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/gpt-series/gpt2/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/gpt-series/gpt2/</guid>
      <description>GPT2 </description>
    </item>
    <item>
      <title>how-to-index-item-ids</title>
      <link>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/p5/how-to-index-item-ids/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/p5/how-to-index-item-ids/</guid>
      <description>How to Index Item IDs 论文How to Index Item IDs for Recommendation Foundation Models的阅读记录。</description>
    </item>
    <item>
      <title>instruction-gpt</title>
      <link>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/gpt-series/instruction-gpt/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/gpt-series/instruction-gpt/</guid>
      <description>Instruction-GPT </description>
    </item>
    <item>
      <title>llama</title>
      <link>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/llama-series/llama/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/llama-series/llama/</guid>
      <description>LLaMa </description>
    </item>
    <item>
      <title>lm-infinite</title>
      <link>https://xcanton.github.io/notes/llm_notes/hullusion-and-training-innovations/lm-infinite/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/hullusion-and-training-innovations/lm-infinite/</guid>
      <description>LM-Infinite 论文《LM-INFINITE: SIMPLE ON-THE-FLY LENGTH GENERALIZATION FOR LARGE LANGUAGE MODELS的阅读记录。&#xA;Objective 针对开源大模型在长文本任务表现不佳的问题，提出对transformer的掩码与距离进行限制，提升模型在长文本生成内容的流畅度和相关性。&#xA;length generalization failure：生成长度超过训练文本长度（通常是预训练的平均长度）的回答。（the typical length in pre-training）</description>
    </item>
    <item>
      <title>lora</title>
      <link>https://xcanton.github.io/notes/llm_notes/tuning/lora/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/tuning/lora/</guid>
      <description>Lora </description>
    </item>
    <item>
      <title>multi-head-attention</title>
      <link>https://xcanton.github.io/notes/llm_notes/before-llm-era/pre-trained-models/attention/multi-head-attention/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/before-llm-era/pre-trained-models/attention/multi-head-attention/</guid>
      <description>Multi-head Attention </description>
    </item>
    <item>
      <title>neftune</title>
      <link>https://xcanton.github.io/notes/llm_notes/tuning/neftune/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/tuning/neftune/</guid>
      <description>NEFTune </description>
    </item>
    <item>
      <title>other-llms</title>
      <link>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/other-llms/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/other-llms/</guid>
      <description>Other LLMs </description>
    </item>
    <item>
      <title>p-tuning</title>
      <link>https://xcanton.github.io/notes/llm_notes/tuning/p-tuning/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/tuning/p-tuning/</guid>
      <description>P-Tuning </description>
    </item>
    <item>
      <title>p-tuning-v2</title>
      <link>https://xcanton.github.io/notes/llm_notes/tuning/p-tuning-v2/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/tuning/p-tuning-v2/</guid>
      <description>P-Tuning V2 </description>
    </item>
    <item>
      <title>ppo</title>
      <link>https://xcanton.github.io/notes/llm_notes/tuning/ppo/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/tuning/ppo/</guid>
      <description>PPO </description>
    </item>
    <item>
      <title>q-lora</title>
      <link>https://xcanton.github.io/notes/llm_notes/tuning/q-lora/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/tuning/q-lora/</guid>
      <description>Q-Lora </description>
    </item>
    <item>
      <title>qwen-v2</title>
      <link>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/qwen/qwen-v2/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/qwen/qwen-v2/</guid>
      <description>Qwen-v2 </description>
    </item>
    <item>
      <title>self-attention</title>
      <link>https://xcanton.github.io/notes/llm_notes/before-llm-era/pre-trained-models/attention/self-attention/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/before-llm-era/pre-trained-models/attention/self-attention/</guid>
      <description>Self-Attention </description>
    </item>
    <item>
      <title>t5</title>
      <link>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/t5/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/t5/</guid>
      <description>T5 论文Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer的阅读记录。</description>
    </item>
    <item>
      <title>transformer</title>
      <link>https://xcanton.github.io/notes/llm_notes/before-llm-era/pre-trained-models/transformer/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/before-llm-era/pre-trained-models/transformer/</guid>
      <description>Transformer </description>
    </item>
    <item>
      <title>tree-based-models-won-in-tabular</title>
      <link>https://xcanton.github.io/notes/llm_notes/other-papers/tabllm/tree-based-models-won-in-tabular/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/other-papers/tabllm/tree-based-models-won-in-tabular/</guid>
      <description>Tree-based models won in tabular 论文Why do tree-based models still outperform deep learning on tabular data?的阅读记录。</description>
    </item>
    <item>
      <title>where-to-go-next-for-recommendation-systems</title>
      <link>https://xcanton.github.io/notes/llm_notes/other-papers/where-to-go-next-for-recommendation-systems/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/other-papers/where-to-go-next-for-recommendation-systems/</guid>
      <description>Where to go Next for Recommendation Systems? 论文Where to Go Next for Recommender Systems? ID- vs. Modality-based Recommender Models Revisited的阅读记录。</description>
    </item>
    <item>
      <title></title>
      <link>https://xcanton.github.io/kaggles/kaggle_notes/enefit-predict-energy-behavior-of-prosumers/fang-an-tiao-yan/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/kaggles/kaggle_notes/enefit-predict-energy-behavior-of-prosumers/fang-an-tiao-yan/</guid>
      <description>方案调研 数据处理 训练方式 模型架构 References </description>
    </item>
    <item>
      <title></title>
      <link>https://xcanton.github.io/kaggles/kaggle_notes/enefit-predict-energy-behavior-of-prosumers/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/kaggles/kaggle_notes/enefit-predict-energy-behavior-of-prosumers/readme/</guid>
      <description>Enefit - Predict Energy Behavior of Prosumers </description>
    </item>
    <item>
      <title></title>
      <link>https://xcanton.github.io/kaggles/kaggle_notes/summary/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/kaggles/kaggle_notes/summary/</guid>
      <description>Table of contents linking-writing-processes-to-writing-quality Enefit - Predict Energy Behavior of Prosumers 方案调研 </description>
    </item>
    <item>
      <title></title>
      <link>https://xcanton.github.io/notes/llm_notes/before-llm-era/pre-trained-models/attention/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/before-llm-era/pre-trained-models/attention/readme/</guid>
      <description>Attention </description>
    </item>
    <item>
      <title></title>
      <link>https://xcanton.github.io/notes/llm_notes/before-llm-era/pre-trained-models/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/before-llm-era/pre-trained-models/readme/</guid>
      <description>Pre-trained Models </description>
    </item>
    <item>
      <title></title>
      <link>https://xcanton.github.io/notes/llm_notes/before-llm-era/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/before-llm-era/readme/</guid>
      <description>🌌 before-llm-era </description>
    </item>
    <item>
      <title></title>
      <link>https://xcanton.github.io/notes/llm_notes/hullusion-and-training-innovations/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/hullusion-and-training-innovations/readme/</guid>
      <description>🤔 hullusion-and-training-innovations </description>
    </item>
    <item>
      <title></title>
      <link>https://xcanton.github.io/notes/llm_notes/knowledge-injection/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/knowledge-injection/readme/</guid>
      <description>knowledge-injection </description>
    </item>
    <item>
      <title></title>
      <link>https://xcanton.github.io/notes/llm_notes/other-papers/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/other-papers/readme/</guid>
      <description>other-papers </description>
    </item>
    <item>
      <title></title>
      <link>https://xcanton.github.io/notes/llm_notes/other-papers/tabllm/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/other-papers/tabllm/readme/</guid>
      <description>TabLLM 论文TabLLM: Few-shot Classification of Tabular Data with Large Language Models的阅读记录。</description>
    </item>
    <item>
      <title></title>
      <link>https://xcanton.github.io/notes/llm_notes/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/readme/</guid>
      <description>_index </description>
    </item>
    <item>
      <title></title>
      <link>https://xcanton.github.io/notes/llm_notes/summary/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/summary/</guid>
      <description>Table of contents _index 🌌 before-llm-era _index Pre-trained Models _index Bert Transformer Attention _index Multi-head Attention Self-Attention 🌅 the-dawn-of-llm-era _index Bloom Other LLMs T5 Baichuan _index Baichuan-v2 ChatGLM _index ChatGLM-v2 ChatGLM-v3 GLM GPT Series _index gpt GPT2 Instruction-GPT GPT4 _index GPT-4 Architecture, Infrastructure,Training Dataset, Costs, Vision, MoE LLaMa Series _index LLaMa P5 _index How to Index Item IDs Qwen _index Qwen-v2 tuning _index DPO Lora NEFTune P-Tuning V2 P-Tuning PPO Q-Lora MOE-Lora 🤔 hullusion-and-training-innovations _index LM-Infinite knowledge-injection _index Can LMs Learn New Entities from Descriptions?</description>
    </item>
    <item>
      <title></title>
      <link>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/baichuan/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/baichuan/readme/</guid>
      <description>Baichuan </description>
    </item>
    <item>
      <title></title>
      <link>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/chatglm/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/chatglm/readme/</guid>
      <description>ChatGLM 论文GLM: General Language Model Pretraining with Autoregressive Blank Infilling的阅读记录。</description>
    </item>
    <item>
      <title></title>
      <link>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/gpt-series/gpt4/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/gpt-series/gpt4/readme/</guid>
      <description>GPT4 </description>
    </item>
    <item>
      <title></title>
      <link>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/gpt-series/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/gpt-series/readme/</guid>
      <description>GPT Series </description>
    </item>
    <item>
      <title></title>
      <link>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/llama-series/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/llama-series/readme/</guid>
      <description>LLaMa Series </description>
    </item>
    <item>
      <title></title>
      <link>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/p5/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/p5/readme/</guid>
      <description>P5 论文Recommendation as Language Processing (RLP): A Unified Pretrain, Personalized Prompt &amp;amp; Predict Paradigm (P5)的阅读记录。</description>
    </item>
    <item>
      <title></title>
      <link>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/qwen/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/qwen/readme/</guid>
      <description>Qwen Qwen Technical Report的阅读记录。</description>
    </item>
    <item>
      <title></title>
      <link>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/readme/</guid>
      <description>🌅 the-dawn-of-llm-era </description>
    </item>
    <item>
      <title></title>
      <link>https://xcanton.github.io/notes/llm_notes/timeseries-prediction-of-llm/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/timeseries-prediction-of-llm/readme/</guid>
      <description>TimeSeries Prediction of LLM 目前大语言模型在时间序列预测的研究主要分成了两个方向:&#xA;一个是LLM -for-TS: 从零开始设计并预训练适用于时间序列的基础大模型, 然后可根据各种下游任务对模型进行微调。这条路径是最基本的解决方案，基于大量数据，通过预训练向模型灌输时间序列相关知识。但时间序列数据更专业且涉及隐私问题，获取大量的时间序列数据困难， 而且由于不同领域的时间序列数据存在重大差异，需要从头开始构建和训练针对不同垂域的各种模型； 一个是 TS-for-LLM: 设计相应机制对时间序列输入大模型进行适配，使其能够适用于现有的语言模型，从而基于现有的语言模型处理时间序列的各类任务。这条路径也具有一定的挑战性，需要超越原始语言模型的能力，补充时间序列语意信息。 两种方向的区别是：&#xA;一种比较自然的方法是将时间序列当成文本序列，但对处理多变量时间序列比较敏感； 第二种是对时间序列进行tokenize，设计一个模块编码TS tokens,并取代原有LLM的embedding layer， 其核心是创建LLM能够理解的ts embedding。 </description>
    </item>
    <item>
      <title></title>
      <link>https://xcanton.github.io/notes/llm_notes/timeseries-prediction-of-llm/ts-for-llm/large-language-models-are-zero-shot-time-series-forecasters-neurips2023/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/timeseries-prediction-of-llm/ts-for-llm/large-language-models-are-zero-shot-time-series-forecasters-neurips2023/</guid>
      <description>Large Language Models Are Zero-Shot Time Series Forecasters [NeurIPS2023] 论文地址：https://arxiv.org/abs/2310.07820 基本的时间预测，通过直接将时序数据用逗号分隔的方式转换成文本输入，通过模型生成其输出以逗号分隔的预测窗口数据。&#xA;文本生成模型是自回归模型，天然将文本token按位循环预测下一输出，与时序预测任务形式相同。 由于是研究Zero-Shot的效果，即在不训练的前提下进行预测，没有训练开销。 与深度模型不同的是，深度模型能够将数值读成一个整体，而文本生成模型由于Tokenizer的原因，对不同的模型和文本有不同的切分形式，所以该文章针对这一问题做出以下修正方案。 针对数字Token的改进 由于部分模型的Tokenizer对数字的切分方式不同，从而导致tokenize后对数字变化的表征不同。为了将数字按位划分，需要在黏着分词器（GPT-3）的输入文本中，将数字调整成空格分隔的形式（如果存在空格+数字的token）。而对非黏着分词器（LLaMa）这么做会导致模型认知不同，导致预测结果不好。&#xA;针对数值范围的改进 由于输入token数量的限制，序列如果本身数值大的会导致token数量变多，与自身变化无关，所以对数值范围进行标准化：&#xA;对LLaMa，论文采用sklearn.preprocessing.MinMaxScaler进行数值缩放（保留8位小数） 论文说GPT-3能够处理不同维度的数值，所以对GPT-3的数据采用的是仿射变换（affine transformation），具体公式如下 $$ (x_1, x_2, &amp;hellip;,x_T):x_t \Rightarrow (x_t - b)/a \ b=min(x_t) - \beta(max(x_i)-min(x_i)) $$&#xA;其中，beta是超参数，alpha是序列按位的百分位数。&#xA;针对随机性的改进 由于是zero-shot，部分模型并不能do_sample=False，导致结果受到随机性影响。所以论文在预测时多次预测（20次）取平均。&#xA;对文本生成的超参数（temperature scaling, logit bias, and nucleus sampling）做网格搜索，GPT搜索alpha、beta、temperature和precision，对LLaMa只搜索temperature，use α = 0.99, β = 0.3, precision = 3, nucleus = 0.9。&#xA;对生成概率改进，由于生成的文本 由于数据已经经过标准化，所以只考虑小数部分的连续概率。&#xA;结论 LLM可以找到数据的低复杂性解释，使他们能够zero-shot外推数值序列</description>
    </item>
    <item>
      <title></title>
      <link>https://xcanton.github.io/notes/llm_notes/timeseries-prediction-of-llm/ts-for-llm/llm-for-time-series-text-prototype-aligned-embedding-to-activate-llms-ability-for-time-series/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/timeseries-prediction-of-llm/ts-for-llm/llm-for-time-series-text-prototype-aligned-embedding-to-activate-llms-ability-for-time-series/</guid>
      <description>LLM for Time Series：Text Prototype Aligned Embedding to Activate LLM’s Ability for Time Series 论文地址：https://arxiv.org/abs/2308.08241 概要</description>
    </item>
    <item>
      <title></title>
      <link>https://xcanton.github.io/notes/llm_notes/timeseries-prediction-of-llm/ts-for-llm/llm4ts-two-stage-fine-tuning-for-time-series-forecasting-with-pre-trained-llms/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/timeseries-prediction-of-llm/ts-for-llm/llm4ts-two-stage-fine-tuning-for-time-series-forecasting-with-pre-trained-llms/</guid>
      <description>LLM4TS: Two-Stage Fine-Tuning for Time-Series Forecasting with Pre-Trained LLMs 论文地址：https://arxiv.org/abs/2308.08469 概要 输入处理类似上一篇文章， 主要尝试解决了两个主要问题： 如何将时间序列数据输入LLM？ 将时间序列转为patch，并通过1d-conv层将每个时序patch转为gpt2 的输入维度大小； 基于look-up 的patch location embedding; 通过channel 独立(通过权重共享间接实现cross-channel交互）的patching， 训练时采用instance-norm，预测时采用RevIN; 将每个patch内的第一个timestamp作为该patch的timestamp， patch内每个timestep的属性进行叠加作为该patch的属性； 如何与现有的LLM进行集成？ 时间序列进行自监督预训练(将LLM适配patch格式的时间序列数据）+下游预测任务微调； 自监督训练时，冻结llm的self-attention和FFN，重新训练输入端和layerNorm; 下游预测任务微调时，先进行linear probing(微调最后一层固定其他）,再微调所有参数的操作 ; 在多个公开数据集上的多变量时间序列预测，few-shot learing 和长时间序列预测超越专家网络模型。 </description>
    </item>
    <item>
      <title></title>
      <link>https://xcanton.github.io/notes/llm_notes/timeseries-prediction-of-llm/ts-for-llm/one-fits-all-power-general-time-series-analysis-by-pretrained-lm/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/timeseries-prediction-of-llm/ts-for-llm/one-fits-all-power-general-time-series-analysis-by-pretrained-lm/</guid>
      <description>One Fits All: Power General Time Series Analysis by Pretrained LM 论文地址：https://arxiv.org/abs/2302.11939 论文代码地址：https://github.com/DAMO-DI-ML/NeurIPS2023-One-Fits-All 概要 输入端先进行先对序列进行RevIN(instanceNorm), 缓解分布漂移； 再参考PatchTST， 将时间序列切割为片段处理，每个片段有postion embedding; 去除token embedding层，将切分的片段输入linear层转为模型需要的输入维度； 冻结编码知识的 multi-head 和FFN层，微调LayerNorm和位置编码； 文章认为基于文本域训练的llm适用于时间序列的一个可能原因是， 自监督的self-attention模块在训练过程中学会了和和具体数据无关的一些运算规则，比如PCA（通过对比对比两个模块的中间结果），使之成为一个广义的计算引擎； 以GPT2为backbone进行了时间序列异常检测、长短期预测等实验，效果均较好; Instance Norm (Channel维度的标准化) Layer Norm（有两个参数） $$ y=\frac {x-\mathbb{E}[x]} {Var[x] + \epsilon} \cdot \gamma + \beta $$</description>
    </item>
    <item>
      <title></title>
      <link>https://xcanton.github.io/notes/llm_notes/timeseries-prediction-of-llm/ts-for-llm/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/timeseries-prediction-of-llm/ts-for-llm/readme/</guid>
      <description>TS-for-LLM </description>
    </item>
    <item>
      <title></title>
      <link>https://xcanton.github.io/notes/llm_notes/timeseries-prediction-of-llm/ts-for-llm/tempo-prompt-based-generative-pre-trained-transformer-for-time-series-forecasting/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/timeseries-prediction-of-llm/ts-for-llm/tempo-prompt-based-generative-pre-trained-transformer-for-time-series-forecasting/</guid>
      <description>TEMPO: prompt-based generative pre-trained transformer for time series forecasting 论文地址：https://arxiv.org/abs/2310.04948 任务定义-序列预测任务 $$ {\hat x}^i_{t},{\hat x}^i_{t+1},\ldots,{\hat x}^i_{t+H-1}=F({x}^i_{t-K},{x}^i_{t-K+1},\ldots,{x}^i_{t-1};{V_i};&#x9;\Phi) $$&#xA;K是特征窗口长度，H是预测序列长度，通过过去K个序列值，预测下H个值 当需要预测多个特征时，i可以为不同的特征编号 $V_i$是对于特征V的prompt，$\phi$是模型参数 与传统文本生成任务不同，Tempo按照时序预测的任务形式构成输入prompt，通过先验规则进行特征工程。&#xA;加入序列先验知识 时序数据构成 $$ X_i=X^i_T + X^i_S+X^i_R $$&#xA;将原数据划分成长期特征、季节性特征和残差特征，一个输入数据由对应的三个特征值相加 长期特征（Trend） $$ X_T \in \mathbb{R}^{n \times L}=\frac {1} {2k+1} \sum^k_{j=-k}(X_{i+j}) $$&#xA;季节性特征（Seasonal，采用局部加权移动平均） $$ {Lowess\ Smoother}() $$&#xA;残差项 $$ X^i_R=X_i-X^i_T - X^i_S $$&#xA;正则化方式 数据构成后 $$ {\hat x}{Tt}^i = \gamma_T(x{Tt}^i-\mathbb{E}[x_{Tt}^{i}]/\sqrt{Var[x_{Tt}^i]+\epsilon_T})+\beta_T $$&#xA;$\mathbb{E}[x_{Tt}^{i}]$是平均数 $Var[x_{Tt}^i]$是标准差 $\gamma_T$和$\beta_T$是可学习的偏移参数 在构成数据之后，每个组成部分都进行reverse instance normalization，能够（促进知识转移，并最大程度得减小转移损失？）。但相对应的，在输出前需要逆向标准化回来。&#xA;预测后 $$ {\widehat Y}{*t}^i=\sqrt{Var[x{Tt}^i]+\epsilon_} \cdot {(\frac{Y_{t}^i-\beta_} {\gamma_})+\mathbb{E}[x_{Tt}^{i}]} $$</description>
    </item>
    <item>
      <title></title>
      <link>https://xcanton.github.io/notes/llm_notes/tuning/moe-lora/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/tuning/moe-lora/</guid>
      <description>MOE-Lora </description>
    </item>
    <item>
      <title></title>
      <link>https://xcanton.github.io/notes/llm_notes/tuning/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/tuning/readme/</guid>
      <description>tuning </description>
    </item>
    <item>
      <title>Math Braintrainer</title>
      <link>https://xcanton.github.io/projects/math-brain-trainer/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/projects/math-brain-trainer/</guid>
      <description></description>
    </item>
    <item>
      <title>Surprise Surprise</title>
      <link>https://xcanton.github.io/articles/article/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/articles/article/</guid>
      <description>Thank you for your support! Hello. If you like this template, I&amp;rsquo;d be happy to get a coffee donation :)</description>
    </item>
  </channel>
</rss>
