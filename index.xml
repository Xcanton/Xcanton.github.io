<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Peixin Xu Peisonal Website</title>
    <link>https://xcanton.github.io/</link>
    <description>Recent content on Peixin Xu Peisonal Website</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Wed, 01 Nov 2023 15:43:17 +0800</lastBuildDate>
    <atom:link href="https://xcanton.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>linking-writing-processes-to-writing-quality</title>
      <link>https://xcanton.github.io/kaggles/kaggle_notes/readme/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:17 +0800</pubDate>
      <guid>https://xcanton.github.io/kaggles/kaggle_notes/readme/</guid>
      <description> description: &amp;gt;- https://www.kaggle.com/competitions/linking-writing-processes-to-writing-quality Linking Writing Processes to Writing Quality </description>
    </item>
    <item>
      <title>baichuan-v2</title>
      <link>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/baichuan/baichuan-v2/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/baichuan/baichuan-v2/</guid>
      <description>Baichuan-v2 </description>
    </item>
    <item>
      <title>bert</title>
      <link>https://xcanton.github.io/notes/llm_notes/before-llm-era/pre-trained-models/bert/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/before-llm-era/pre-trained-models/bert/</guid>
      <description>Bert </description>
    </item>
    <item>
      <title>bloom</title>
      <link>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/bloom/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/bloom/</guid>
      <description>Bloom è®ºæ–‡BLOOM: A 176B-Parameter Open-Access Multilingual Language Modelçš„é˜…è¯»è®°å½•ã€‚</description>
    </item>
    <item>
      <title>can-lms-learn-new-entities-from-descriptions</title>
      <link>https://xcanton.github.io/notes/llm_notes/knowledge-injection/can-lms-learn-new-entities-from-descriptions/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/knowledge-injection/can-lms-learn-new-entities-from-descriptions/</guid>
      <description>Can LMs Learn New Entities from Descriptions? è®ºæ–‡Can LMs Learn New Entities from Descriptions? Challenges in Propagating Injected Knowledgeçš„é˜…è¯»è®°å½•ã€‚</description>
    </item>
    <item>
      <title>chatglm-v2</title>
      <link>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/chatglm/chatglm-v2/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/chatglm/chatglm-v2/</guid>
      <description>ChatGLM-v2 </description>
    </item>
    <item>
      <title>chatglm-v3</title>
      <link>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/chatglm/chatglm-v3/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/chatglm/chatglm-v3/</guid>
      <description>ChatGLM-v3 </description>
    </item>
    <item>
      <title>dpo</title>
      <link>https://xcanton.github.io/notes/llm_notes/tuning/dpo/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/tuning/dpo/</guid>
      <description>DPO </description>
    </item>
    <item>
      <title>flan</title>
      <link>https://xcanton.github.io/notes/llm_notes/other-papers/flan/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/other-papers/flan/</guid>
      <description>Flan è®ºæ–‡The Flan Collection: Designing Data and Methods for Effective Instruction Tuningçš„é˜…è¯»è®°å½•ã€‚</description>
    </item>
    <item>
      <title>glm</title>
      <link>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/chatglm/glm/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/chatglm/glm/</guid>
      <description>GLM è®ºæ–‡GLM-130B: An Open Bilingual Pre-trained Modelçš„é˜…è¯»è®°å½•ã€‚</description>
    </item>
    <item>
      <title>gpt</title>
      <link>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/gpt-series/gpt/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/gpt-series/gpt/</guid>
      <description> description: &amp;lsquo;Paper :: Improving Language Understanding by Generative Pre-Training&amp;rsquo; GPT </description>
    </item>
    <item>
      <title>gpt-4-architecture-infrastructure-training-dataset-costs-vision-moe</title>
      <link>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/gpt-series/gpt4/gpt-4-architecture-infrastructure-training-dataset-costs-vision-moe/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/gpt-series/gpt4/gpt-4-architecture-infrastructure-training-dataset-costs-vision-moe/</guid>
      <description>GPT-4 Architecture, Infrastructure,Training Dataset, Costs, Vision, MoE </description>
    </item>
    <item>
      <title>gpt2</title>
      <link>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/gpt-series/gpt2/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/gpt-series/gpt2/</guid>
      <description>GPT2 </description>
    </item>
    <item>
      <title>how-to-index-item-ids</title>
      <link>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/p5/how-to-index-item-ids/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/p5/how-to-index-item-ids/</guid>
      <description>How to Index Item IDs è®ºæ–‡How to Index Item IDs for Recommendation Foundation Modelsçš„é˜…è¯»è®°å½•ã€‚</description>
    </item>
    <item>
      <title>instruction-gpt</title>
      <link>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/gpt-series/instruction-gpt/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/gpt-series/instruction-gpt/</guid>
      <description>Instruction-GPT </description>
    </item>
    <item>
      <title>llama</title>
      <link>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/llama-series/llama/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/llama-series/llama/</guid>
      <description>LLaMa </description>
    </item>
    <item>
      <title>lm-infinite</title>
      <link>https://xcanton.github.io/notes/llm_notes/hullusion-and-training-innovations/lm-infinite/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/hullusion-and-training-innovations/lm-infinite/</guid>
      <description>LM-Infinite è®ºæ–‡ã€ŠLM-INFINITE: SIMPLE ON-THE-FLY LENGTH GENERALIZATION FOR LARGE LANGUAGE MODELSçš„é˜…è¯»è®°å½•ã€‚&#xA;Objective é’ˆå¯¹å¼€æºå¤§æ¨¡å‹åœ¨é•¿æ–‡æœ¬ä»»åŠ¡è¡¨ç°ä¸ä½³çš„é—®é¢˜ï¼Œæå‡ºå¯¹transformerçš„æ©ç ä¸è·ç¦»è¿›è¡Œé™åˆ¶ï¼Œæå‡æ¨¡å‹åœ¨é•¿æ–‡æœ¬ç”Ÿæˆå†…å®¹çš„æµç•…åº¦å’Œç›¸å…³æ€§ã€‚&#xA;length generalization failureï¼šç”Ÿæˆé•¿åº¦è¶…è¿‡è®­ç»ƒæ–‡æœ¬é•¿åº¦ï¼ˆé€šå¸¸æ˜¯é¢„è®­ç»ƒçš„å¹³å‡é•¿åº¦ï¼‰çš„å›ç­”ã€‚ï¼ˆthe typical length in pre-trainingï¼‰</description>
    </item>
    <item>
      <title>lora</title>
      <link>https://xcanton.github.io/notes/llm_notes/tuning/lora/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/tuning/lora/</guid>
      <description>Lora </description>
    </item>
    <item>
      <title>multi-head-attention</title>
      <link>https://xcanton.github.io/notes/llm_notes/before-llm-era/pre-trained-models/attention/multi-head-attention/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/before-llm-era/pre-trained-models/attention/multi-head-attention/</guid>
      <description>Multi-head Attention </description>
    </item>
    <item>
      <title>neftune</title>
      <link>https://xcanton.github.io/notes/llm_notes/tuning/neftune/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/tuning/neftune/</guid>
      <description>NEFTune </description>
    </item>
    <item>
      <title>other-llms</title>
      <link>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/other-llms/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/other-llms/</guid>
      <description>Other LLMs </description>
    </item>
    <item>
      <title>p-tuning</title>
      <link>https://xcanton.github.io/notes/llm_notes/tuning/p-tuning/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/tuning/p-tuning/</guid>
      <description>P-Tuning </description>
    </item>
    <item>
      <title>p-tuning-v2</title>
      <link>https://xcanton.github.io/notes/llm_notes/tuning/p-tuning-v2/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/tuning/p-tuning-v2/</guid>
      <description>P-Tuning V2 </description>
    </item>
    <item>
      <title>ppo</title>
      <link>https://xcanton.github.io/notes/llm_notes/tuning/ppo/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/tuning/ppo/</guid>
      <description>PPO </description>
    </item>
    <item>
      <title>q-lora</title>
      <link>https://xcanton.github.io/notes/llm_notes/tuning/q-lora/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/tuning/q-lora/</guid>
      <description>Q-Lora </description>
    </item>
    <item>
      <title>qwen-v2</title>
      <link>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/qwen/qwen-v2/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/qwen/qwen-v2/</guid>
      <description>Qwen-v2 </description>
    </item>
    <item>
      <title>self-attention</title>
      <link>https://xcanton.github.io/notes/llm_notes/before-llm-era/pre-trained-models/attention/self-attention/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/before-llm-era/pre-trained-models/attention/self-attention/</guid>
      <description>Self-Attention </description>
    </item>
    <item>
      <title>t5</title>
      <link>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/t5/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/t5/</guid>
      <description>T5 è®ºæ–‡Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformerçš„é˜…è¯»è®°å½•ã€‚</description>
    </item>
    <item>
      <title>transformer</title>
      <link>https://xcanton.github.io/notes/llm_notes/before-llm-era/pre-trained-models/transformer/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/before-llm-era/pre-trained-models/transformer/</guid>
      <description>Transformer </description>
    </item>
    <item>
      <title>tree-based-models-won-in-tabular</title>
      <link>https://xcanton.github.io/notes/llm_notes/other-papers/tabllm/tree-based-models-won-in-tabular/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/other-papers/tabllm/tree-based-models-won-in-tabular/</guid>
      <description>Tree-based models won in tabular è®ºæ–‡Why do tree-based models still outperform deep learning on tabular data?çš„é˜…è¯»è®°å½•ã€‚</description>
    </item>
    <item>
      <title>where-to-go-next-for-recommendation-systems</title>
      <link>https://xcanton.github.io/notes/llm_notes/other-papers/where-to-go-next-for-recommendation-systems/</link>
      <pubDate>Wed, 01 Nov 2023 15:43:13 +0800</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/other-papers/where-to-go-next-for-recommendation-systems/</guid>
      <description>Where to go Next for Recommendation Systems? è®ºæ–‡Where to Go Next for Recommender Systems? ID- vs. Modality-based Recommender Models Revisitedçš„é˜…è¯»è®°å½•ã€‚</description>
    </item>
    <item>
      <title></title>
      <link>https://xcanton.github.io/kaggles/kaggle_notes/enefit-predict-energy-behavior-of-prosumers/fang-an-tiao-yan/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/kaggles/kaggle_notes/enefit-predict-energy-behavior-of-prosumers/fang-an-tiao-yan/</guid>
      <description>æ–¹æ¡ˆè°ƒç ” æ•°æ®å¤„ç† è®­ç»ƒæ–¹å¼ æ¨¡å‹æ¶æ„ References </description>
    </item>
    <item>
      <title></title>
      <link>https://xcanton.github.io/kaggles/kaggle_notes/enefit-predict-energy-behavior-of-prosumers/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/kaggles/kaggle_notes/enefit-predict-energy-behavior-of-prosumers/readme/</guid>
      <description>Enefit - Predict Energy Behavior of Prosumers </description>
    </item>
    <item>
      <title></title>
      <link>https://xcanton.github.io/kaggles/kaggle_notes/summary/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/kaggles/kaggle_notes/summary/</guid>
      <description>Table of contents linking-writing-processes-to-writing-quality Enefit - Predict Energy Behavior of Prosumers æ–¹æ¡ˆè°ƒç ” </description>
    </item>
    <item>
      <title></title>
      <link>https://xcanton.github.io/notes/llm_notes/before-llm-era/pre-trained-models/attention/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/before-llm-era/pre-trained-models/attention/readme/</guid>
      <description>Attention </description>
    </item>
    <item>
      <title></title>
      <link>https://xcanton.github.io/notes/llm_notes/before-llm-era/pre-trained-models/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/before-llm-era/pre-trained-models/readme/</guid>
      <description>Pre-trained Models </description>
    </item>
    <item>
      <title></title>
      <link>https://xcanton.github.io/notes/llm_notes/before-llm-era/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/before-llm-era/readme/</guid>
      <description>ğŸŒŒ before-llm-era </description>
    </item>
    <item>
      <title></title>
      <link>https://xcanton.github.io/notes/llm_notes/hullusion-and-training-innovations/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/hullusion-and-training-innovations/readme/</guid>
      <description>ğŸ¤” hullusion-and-training-innovations </description>
    </item>
    <item>
      <title></title>
      <link>https://xcanton.github.io/notes/llm_notes/knowledge-injection/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/knowledge-injection/readme/</guid>
      <description>knowledge-injection </description>
    </item>
    <item>
      <title></title>
      <link>https://xcanton.github.io/notes/llm_notes/other-papers/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/other-papers/readme/</guid>
      <description>other-papers </description>
    </item>
    <item>
      <title></title>
      <link>https://xcanton.github.io/notes/llm_notes/other-papers/tabllm/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/other-papers/tabllm/readme/</guid>
      <description>TabLLM è®ºæ–‡TabLLM: Few-shot Classification of Tabular Data with Large Language Modelsçš„é˜…è¯»è®°å½•ã€‚</description>
    </item>
    <item>
      <title></title>
      <link>https://xcanton.github.io/notes/llm_notes/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/readme/</guid>
      <description>_index </description>
    </item>
    <item>
      <title></title>
      <link>https://xcanton.github.io/notes/llm_notes/summary/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/summary/</guid>
      <description>Table of contents _index ğŸŒŒ before-llm-era _index Pre-trained Models _index Bert Transformer Attention _index Multi-head Attention Self-Attention ğŸŒ… the-dawn-of-llm-era _index Bloom Other LLMs T5 Baichuan _index Baichuan-v2 ChatGLM _index ChatGLM-v2 ChatGLM-v3 GLM GPT Series _index gpt GPT2 Instruction-GPT GPT4 _index GPT-4 Architecture, Infrastructure,Training Dataset, Costs, Vision, MoE LLaMa Series _index LLaMa P5 _index How to Index Item IDs Qwen _index Qwen-v2 tuning _index DPO Lora NEFTune P-Tuning V2 P-Tuning PPO Q-Lora MOE-Lora ğŸ¤” hullusion-and-training-innovations _index LM-Infinite knowledge-injection _index Can LMs Learn New Entities from Descriptions?</description>
    </item>
    <item>
      <title></title>
      <link>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/baichuan/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/baichuan/readme/</guid>
      <description>Baichuan </description>
    </item>
    <item>
      <title></title>
      <link>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/chatglm/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/chatglm/readme/</guid>
      <description>ChatGLM è®ºæ–‡GLM: General Language Model Pretraining with Autoregressive Blank Infillingçš„é˜…è¯»è®°å½•ã€‚</description>
    </item>
    <item>
      <title></title>
      <link>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/gpt-series/gpt4/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/gpt-series/gpt4/readme/</guid>
      <description>GPT4 </description>
    </item>
    <item>
      <title></title>
      <link>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/gpt-series/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/gpt-series/readme/</guid>
      <description>GPT Series </description>
    </item>
    <item>
      <title></title>
      <link>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/llama-series/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/llama-series/readme/</guid>
      <description>LLaMa Series </description>
    </item>
    <item>
      <title></title>
      <link>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/p5/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/p5/readme/</guid>
      <description>P5 è®ºæ–‡Recommendation as Language Processing (RLP): A Unified Pretrain, Personalized Prompt &amp;amp; Predict Paradigm (P5)çš„é˜…è¯»è®°å½•ã€‚</description>
    </item>
    <item>
      <title></title>
      <link>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/qwen/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/qwen/readme/</guid>
      <description>Qwen Qwen Technical Reportçš„é˜…è¯»è®°å½•ã€‚</description>
    </item>
    <item>
      <title></title>
      <link>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/the-dawn-of-llm-era/readme/</guid>
      <description>ğŸŒ… the-dawn-of-llm-era </description>
    </item>
    <item>
      <title></title>
      <link>https://xcanton.github.io/notes/llm_notes/timeseries-prediction-of-llm/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/timeseries-prediction-of-llm/readme/</guid>
      <description>TimeSeries Prediction of LLM ç›®å‰å¤§è¯­è¨€æ¨¡å‹åœ¨æ—¶é—´åºåˆ—é¢„æµ‹çš„ç ”ç©¶ä¸»è¦åˆ†æˆäº†ä¸¤ä¸ªæ–¹å‘:&#xA;ä¸€ä¸ªæ˜¯LLM -for-TS: ä»é›¶å¼€å§‹è®¾è®¡å¹¶é¢„è®­ç»ƒé€‚ç”¨äºæ—¶é—´åºåˆ—çš„åŸºç¡€å¤§æ¨¡å‹, ç„¶åå¯æ ¹æ®å„ç§ä¸‹æ¸¸ä»»åŠ¡å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚è¿™æ¡è·¯å¾„æ˜¯æœ€åŸºæœ¬çš„è§£å†³æ–¹æ¡ˆï¼ŒåŸºäºå¤§é‡æ•°æ®ï¼Œé€šè¿‡é¢„è®­ç»ƒå‘æ¨¡å‹çŒè¾“æ—¶é—´åºåˆ—ç›¸å…³çŸ¥è¯†ã€‚ä½†æ—¶é—´åºåˆ—æ•°æ®æ›´ä¸“ä¸šä¸”æ¶‰åŠéšç§é—®é¢˜ï¼Œè·å–å¤§é‡çš„æ—¶é—´åºåˆ—æ•°æ®å›°éš¾ï¼Œ è€Œä¸”ç”±äºä¸åŒé¢†åŸŸçš„æ—¶é—´åºåˆ—æ•°æ®å­˜åœ¨é‡å¤§å·®å¼‚ï¼Œéœ€è¦ä»å¤´å¼€å§‹æ„å»ºå’Œè®­ç»ƒé’ˆå¯¹ä¸åŒå‚åŸŸçš„å„ç§æ¨¡å‹ï¼› ä¸€ä¸ªæ˜¯ TS-for-LLM: è®¾è®¡ç›¸åº”æœºåˆ¶å¯¹æ—¶é—´åºåˆ—è¾“å…¥å¤§æ¨¡å‹è¿›è¡Œé€‚é…ï¼Œä½¿å…¶èƒ½å¤Ÿé€‚ç”¨äºç°æœ‰çš„è¯­è¨€æ¨¡å‹ï¼Œä»è€ŒåŸºäºç°æœ‰çš„è¯­è¨€æ¨¡å‹å¤„ç†æ—¶é—´åºåˆ—çš„å„ç±»ä»»åŠ¡ã€‚è¿™æ¡è·¯å¾„ä¹Ÿå…·æœ‰ä¸€å®šçš„æŒ‘æˆ˜æ€§ï¼Œéœ€è¦è¶…è¶ŠåŸå§‹è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œè¡¥å……æ—¶é—´åºåˆ—è¯­æ„ä¿¡æ¯ã€‚ ä¸¤ç§æ–¹å‘çš„åŒºåˆ«æ˜¯ï¼š&#xA;ä¸€ç§æ¯”è¾ƒè‡ªç„¶çš„æ–¹æ³•æ˜¯å°†æ—¶é—´åºåˆ—å½“æˆæ–‡æœ¬åºåˆ—ï¼Œä½†å¯¹å¤„ç†å¤šå˜é‡æ—¶é—´åºåˆ—æ¯”è¾ƒæ•æ„Ÿï¼› ç¬¬äºŒç§æ˜¯å¯¹æ—¶é—´åºåˆ—è¿›è¡Œtokenizeï¼Œè®¾è®¡ä¸€ä¸ªæ¨¡å—ç¼–ç TS tokens,å¹¶å–ä»£åŸæœ‰LLMçš„embedding layerï¼Œ å…¶æ ¸å¿ƒæ˜¯åˆ›å»ºLLMèƒ½å¤Ÿç†è§£çš„ts embeddingã€‚ </description>
    </item>
    <item>
      <title></title>
      <link>https://xcanton.github.io/notes/llm_notes/timeseries-prediction-of-llm/ts-for-llm/large-language-models-are-zero-shot-time-series-forecasters-neurips2023/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/timeseries-prediction-of-llm/ts-for-llm/large-language-models-are-zero-shot-time-series-forecasters-neurips2023/</guid>
      <description>Large Language Models Are Zero-Shot Time Series Forecasters [NeurIPS2023] è®ºæ–‡åœ°å€ï¼šhttps://arxiv.org/abs/2310.07820 åŸºæœ¬çš„æ—¶é—´é¢„æµ‹ï¼Œé€šè¿‡ç›´æ¥å°†æ—¶åºæ•°æ®ç”¨é€—å·åˆ†éš”çš„æ–¹å¼è½¬æ¢æˆæ–‡æœ¬è¾“å…¥ï¼Œé€šè¿‡æ¨¡å‹ç”Ÿæˆå…¶è¾“å‡ºä»¥é€—å·åˆ†éš”çš„é¢„æµ‹çª—å£æ•°æ®ã€‚&#xA;æ–‡æœ¬ç”Ÿæˆæ¨¡å‹æ˜¯è‡ªå›å½’æ¨¡å‹ï¼Œå¤©ç„¶å°†æ–‡æœ¬tokenæŒ‰ä½å¾ªç¯é¢„æµ‹ä¸‹ä¸€è¾“å‡ºï¼Œä¸æ—¶åºé¢„æµ‹ä»»åŠ¡å½¢å¼ç›¸åŒã€‚ ç”±äºæ˜¯ç ”ç©¶Zero-Shotçš„æ•ˆæœï¼Œå³åœ¨ä¸è®­ç»ƒçš„å‰æä¸‹è¿›è¡Œé¢„æµ‹ï¼Œæ²¡æœ‰è®­ç»ƒå¼€é”€ã€‚ ä¸æ·±åº¦æ¨¡å‹ä¸åŒçš„æ˜¯ï¼Œæ·±åº¦æ¨¡å‹èƒ½å¤Ÿå°†æ•°å€¼è¯»æˆä¸€ä¸ªæ•´ä½“ï¼Œè€Œæ–‡æœ¬ç”Ÿæˆæ¨¡å‹ç”±äºTokenizerçš„åŸå› ï¼Œå¯¹ä¸åŒçš„æ¨¡å‹å’Œæ–‡æœ¬æœ‰ä¸åŒçš„åˆ‡åˆ†å½¢å¼ï¼Œæ‰€ä»¥è¯¥æ–‡ç« é’ˆå¯¹è¿™ä¸€é—®é¢˜åšå‡ºä»¥ä¸‹ä¿®æ­£æ–¹æ¡ˆã€‚ é’ˆå¯¹æ•°å­—Tokençš„æ”¹è¿› ç”±äºéƒ¨åˆ†æ¨¡å‹çš„Tokenizerå¯¹æ•°å­—çš„åˆ‡åˆ†æ–¹å¼ä¸åŒï¼Œä»è€Œå¯¼è‡´tokenizeåå¯¹æ•°å­—å˜åŒ–çš„è¡¨å¾ä¸åŒã€‚ä¸ºäº†å°†æ•°å­—æŒ‰ä½åˆ’åˆ†ï¼Œéœ€è¦åœ¨é»ç€åˆ†è¯å™¨ï¼ˆGPT-3ï¼‰çš„è¾“å…¥æ–‡æœ¬ä¸­ï¼Œå°†æ•°å­—è°ƒæ•´æˆç©ºæ ¼åˆ†éš”çš„å½¢å¼ï¼ˆå¦‚æœå­˜åœ¨ç©ºæ ¼+æ•°å­—çš„tokenï¼‰ã€‚è€Œå¯¹éé»ç€åˆ†è¯å™¨ï¼ˆLLaMaï¼‰è¿™ä¹ˆåšä¼šå¯¼è‡´æ¨¡å‹è®¤çŸ¥ä¸åŒï¼Œå¯¼è‡´é¢„æµ‹ç»“æœä¸å¥½ã€‚&#xA;é’ˆå¯¹æ•°å€¼èŒƒå›´çš„æ”¹è¿› ç”±äºè¾“å…¥tokenæ•°é‡çš„é™åˆ¶ï¼Œåºåˆ—å¦‚æœæœ¬èº«æ•°å€¼å¤§çš„ä¼šå¯¼è‡´tokenæ•°é‡å˜å¤šï¼Œä¸è‡ªèº«å˜åŒ–æ— å…³ï¼Œæ‰€ä»¥å¯¹æ•°å€¼èŒƒå›´è¿›è¡Œæ ‡å‡†åŒ–ï¼š&#xA;å¯¹LLaMaï¼Œè®ºæ–‡é‡‡ç”¨sklearn.preprocessing.MinMaxScalerè¿›è¡Œæ•°å€¼ç¼©æ”¾ï¼ˆä¿ç•™8ä½å°æ•°ï¼‰ è®ºæ–‡è¯´GPT-3èƒ½å¤Ÿå¤„ç†ä¸åŒç»´åº¦çš„æ•°å€¼ï¼Œæ‰€ä»¥å¯¹GPT-3çš„æ•°æ®é‡‡ç”¨çš„æ˜¯ä»¿å°„å˜æ¢ï¼ˆaffine transformationï¼‰ï¼Œå…·ä½“å…¬å¼å¦‚ä¸‹ $$ (x_1, x_2, &amp;hellip;,x_T):x_t \Rightarrow (x_t - b)/a \ b=min(x_t) - \beta(max(x_i)-min(x_i)) $$&#xA;å…¶ä¸­ï¼Œbetaæ˜¯è¶…å‚æ•°ï¼Œalphaæ˜¯åºåˆ—æŒ‰ä½çš„ç™¾åˆ†ä½æ•°ã€‚&#xA;é’ˆå¯¹éšæœºæ€§çš„æ”¹è¿› ç”±äºæ˜¯zero-shotï¼Œéƒ¨åˆ†æ¨¡å‹å¹¶ä¸èƒ½do_sample=Falseï¼Œå¯¼è‡´ç»“æœå—åˆ°éšæœºæ€§å½±å“ã€‚æ‰€ä»¥è®ºæ–‡åœ¨é¢„æµ‹æ—¶å¤šæ¬¡é¢„æµ‹ï¼ˆ20æ¬¡ï¼‰å–å¹³å‡ã€‚&#xA;å¯¹æ–‡æœ¬ç”Ÿæˆçš„è¶…å‚æ•°ï¼ˆtemperature scaling, logit bias, and nucleus samplingï¼‰åšç½‘æ ¼æœç´¢ï¼ŒGPTæœç´¢alphaã€betaã€temperatureå’Œprecisionï¼Œå¯¹LLaMaåªæœç´¢temperatureï¼Œuse Î± = 0.99, Î² = 0.3, precision = 3, nucleus = 0.9ã€‚&#xA;å¯¹ç”Ÿæˆæ¦‚ç‡æ”¹è¿›ï¼Œç”±äºç”Ÿæˆçš„æ–‡æœ¬ ç”±äºæ•°æ®å·²ç»ç»è¿‡æ ‡å‡†åŒ–ï¼Œæ‰€ä»¥åªè€ƒè™‘å°æ•°éƒ¨åˆ†çš„è¿ç»­æ¦‚ç‡ã€‚&#xA;ç»“è®º LLMå¯ä»¥æ‰¾åˆ°æ•°æ®çš„ä½å¤æ‚æ€§è§£é‡Šï¼Œä½¿ä»–ä»¬èƒ½å¤Ÿzero-shotå¤–æ¨æ•°å€¼åºåˆ—</description>
    </item>
    <item>
      <title></title>
      <link>https://xcanton.github.io/notes/llm_notes/timeseries-prediction-of-llm/ts-for-llm/llm-for-time-series-text-prototype-aligned-embedding-to-activate-llms-ability-for-time-series/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/timeseries-prediction-of-llm/ts-for-llm/llm-for-time-series-text-prototype-aligned-embedding-to-activate-llms-ability-for-time-series/</guid>
      <description>LLM for Time Seriesï¼šText Prototype Aligned Embedding to Activate LLMâ€™s Ability for Time Series è®ºæ–‡åœ°å€ï¼šhttps://arxiv.org/abs/2308.08241 æ¦‚è¦</description>
    </item>
    <item>
      <title></title>
      <link>https://xcanton.github.io/notes/llm_notes/timeseries-prediction-of-llm/ts-for-llm/llm4ts-two-stage-fine-tuning-for-time-series-forecasting-with-pre-trained-llms/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/timeseries-prediction-of-llm/ts-for-llm/llm4ts-two-stage-fine-tuning-for-time-series-forecasting-with-pre-trained-llms/</guid>
      <description>LLM4TS: Two-Stage Fine-Tuning for Time-Series Forecasting with Pre-Trained LLMs è®ºæ–‡åœ°å€ï¼šhttps://arxiv.org/abs/2308.08469 æ¦‚è¦ è¾“å…¥å¤„ç†ç±»ä¼¼ä¸Šä¸€ç¯‡æ–‡ç« ï¼Œ ä¸»è¦å°è¯•è§£å†³äº†ä¸¤ä¸ªä¸»è¦é—®é¢˜ï¼š å¦‚ä½•å°†æ—¶é—´åºåˆ—æ•°æ®è¾“å…¥LLMï¼Ÿ å°†æ—¶é—´åºåˆ—è½¬ä¸ºpatchï¼Œå¹¶é€šè¿‡1d-convå±‚å°†æ¯ä¸ªæ—¶åºpatchè½¬ä¸ºgpt2 çš„è¾“å…¥ç»´åº¦å¤§å°ï¼› åŸºäºlook-up çš„patch location embedding; é€šè¿‡channel ç‹¬ç«‹(é€šè¿‡æƒé‡å…±äº«é—´æ¥å®ç°cross-channeläº¤äº’ï¼‰çš„patchingï¼Œ è®­ç»ƒæ—¶é‡‡ç”¨instance-normï¼Œé¢„æµ‹æ—¶é‡‡ç”¨RevIN; å°†æ¯ä¸ªpatchå†…çš„ç¬¬ä¸€ä¸ªtimestampä½œä¸ºè¯¥patchçš„timestampï¼Œ patchå†…æ¯ä¸ªtimestepçš„å±æ€§è¿›è¡Œå åŠ ä½œä¸ºè¯¥patchçš„å±æ€§ï¼› å¦‚ä½•ä¸ç°æœ‰çš„LLMè¿›è¡Œé›†æˆï¼Ÿ æ—¶é—´åºåˆ—è¿›è¡Œè‡ªç›‘ç£é¢„è®­ç»ƒ(å°†LLMé€‚é…patchæ ¼å¼çš„æ—¶é—´åºåˆ—æ•°æ®ï¼‰+ä¸‹æ¸¸é¢„æµ‹ä»»åŠ¡å¾®è°ƒï¼› è‡ªç›‘ç£è®­ç»ƒæ—¶ï¼Œå†»ç»“llmçš„self-attentionå’ŒFFNï¼Œé‡æ–°è®­ç»ƒè¾“å…¥ç«¯å’ŒlayerNorm; ä¸‹æ¸¸é¢„æµ‹ä»»åŠ¡å¾®è°ƒæ—¶ï¼Œå…ˆè¿›è¡Œlinear probing(å¾®è°ƒæœ€åä¸€å±‚å›ºå®šå…¶ä»–ï¼‰,å†å¾®è°ƒæ‰€æœ‰å‚æ•°çš„æ“ä½œ ; åœ¨å¤šä¸ªå…¬å¼€æ•°æ®é›†ä¸Šçš„å¤šå˜é‡æ—¶é—´åºåˆ—é¢„æµ‹ï¼Œfew-shot learing å’Œé•¿æ—¶é—´åºåˆ—é¢„æµ‹è¶…è¶Šä¸“å®¶ç½‘ç»œæ¨¡å‹ã€‚ </description>
    </item>
    <item>
      <title></title>
      <link>https://xcanton.github.io/notes/llm_notes/timeseries-prediction-of-llm/ts-for-llm/one-fits-all-power-general-time-series-analysis-by-pretrained-lm/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/timeseries-prediction-of-llm/ts-for-llm/one-fits-all-power-general-time-series-analysis-by-pretrained-lm/</guid>
      <description>One Fits All: Power General Time Series Analysis by Pretrained LM è®ºæ–‡åœ°å€ï¼šhttps://arxiv.org/abs/2302.11939 è®ºæ–‡ä»£ç åœ°å€ï¼šhttps://github.com/DAMO-DI-ML/NeurIPS2023-One-Fits-All æ¦‚è¦ è¾“å…¥ç«¯å…ˆè¿›è¡Œå…ˆå¯¹åºåˆ—è¿›è¡ŒRevIN(instanceNorm), ç¼“è§£åˆ†å¸ƒæ¼‚ç§»ï¼› å†å‚è€ƒPatchTSTï¼Œ å°†æ—¶é—´åºåˆ—åˆ‡å‰²ä¸ºç‰‡æ®µå¤„ç†ï¼Œæ¯ä¸ªç‰‡æ®µæœ‰postion embedding; å»é™¤token embeddingå±‚ï¼Œå°†åˆ‡åˆ†çš„ç‰‡æ®µè¾“å…¥linearå±‚è½¬ä¸ºæ¨¡å‹éœ€è¦çš„è¾“å…¥ç»´åº¦ï¼› å†»ç»“ç¼–ç çŸ¥è¯†çš„ multi-head å’ŒFFNå±‚ï¼Œå¾®è°ƒLayerNormå’Œä½ç½®ç¼–ç ï¼› æ–‡ç« è®¤ä¸ºåŸºäºæ–‡æœ¬åŸŸè®­ç»ƒçš„llmé€‚ç”¨äºæ—¶é—´åºåˆ—çš„ä¸€ä¸ªå¯èƒ½åŸå› æ˜¯ï¼Œ è‡ªç›‘ç£çš„self-attentionæ¨¡å—åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å­¦ä¼šäº†å’Œå’Œå…·ä½“æ•°æ®æ— å…³çš„ä¸€äº›è¿ç®—è§„åˆ™ï¼Œæ¯”å¦‚PCAï¼ˆé€šè¿‡å¯¹æ¯”å¯¹æ¯”ä¸¤ä¸ªæ¨¡å—çš„ä¸­é—´ç»“æœï¼‰ï¼Œä½¿ä¹‹æˆä¸ºä¸€ä¸ªå¹¿ä¹‰çš„è®¡ç®—å¼•æ“ï¼› ä»¥GPT2ä¸ºbackboneè¿›è¡Œäº†æ—¶é—´åºåˆ—å¼‚å¸¸æ£€æµ‹ã€é•¿çŸ­æœŸé¢„æµ‹ç­‰å®éªŒï¼Œæ•ˆæœå‡è¾ƒå¥½; Instance Norm (Channelç»´åº¦çš„æ ‡å‡†åŒ–) Layer Normï¼ˆæœ‰ä¸¤ä¸ªå‚æ•°ï¼‰ $$ y=\frac {x-\mathbb{E}[x]} {Var[x] + \epsilon} \cdot \gamma + \beta $$</description>
    </item>
    <item>
      <title></title>
      <link>https://xcanton.github.io/notes/llm_notes/timeseries-prediction-of-llm/ts-for-llm/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/timeseries-prediction-of-llm/ts-for-llm/readme/</guid>
      <description>TS-for-LLM </description>
    </item>
    <item>
      <title></title>
      <link>https://xcanton.github.io/notes/llm_notes/timeseries-prediction-of-llm/ts-for-llm/tempo-prompt-based-generative-pre-trained-transformer-for-time-series-forecasting/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/timeseries-prediction-of-llm/ts-for-llm/tempo-prompt-based-generative-pre-trained-transformer-for-time-series-forecasting/</guid>
      <description>TEMPO: prompt-based generative pre-trained transformer for time series forecasting è®ºæ–‡åœ°å€ï¼šhttps://arxiv.org/abs/2310.04948 ä»»åŠ¡å®šä¹‰-åºåˆ—é¢„æµ‹ä»»åŠ¡ $$ {\hat x}^i_{t},{\hat x}^i_{t+1},\ldots,{\hat x}^i_{t+H-1}=F({x}^i_{t-K},{x}^i_{t-K+1},\ldots,{x}^i_{t-1};{V_i};&#x9;\Phi) $$&#xA;Kæ˜¯ç‰¹å¾çª—å£é•¿åº¦ï¼ŒHæ˜¯é¢„æµ‹åºåˆ—é•¿åº¦ï¼Œé€šè¿‡è¿‡å»Kä¸ªåºåˆ—å€¼ï¼Œé¢„æµ‹ä¸‹Hä¸ªå€¼ å½“éœ€è¦é¢„æµ‹å¤šä¸ªç‰¹å¾æ—¶ï¼Œiå¯ä»¥ä¸ºä¸åŒçš„ç‰¹å¾ç¼–å· $V_i$æ˜¯å¯¹äºç‰¹å¾Vçš„promptï¼Œ$\phi$æ˜¯æ¨¡å‹å‚æ•° ä¸ä¼ ç»Ÿæ–‡æœ¬ç”Ÿæˆä»»åŠ¡ä¸åŒï¼ŒTempoæŒ‰ç…§æ—¶åºé¢„æµ‹çš„ä»»åŠ¡å½¢å¼æ„æˆè¾“å…¥promptï¼Œé€šè¿‡å…ˆéªŒè§„åˆ™è¿›è¡Œç‰¹å¾å·¥ç¨‹ã€‚&#xA;åŠ å…¥åºåˆ—å…ˆéªŒçŸ¥è¯† æ—¶åºæ•°æ®æ„æˆ $$ X_i=X^i_T + X^i_S+X^i_R $$&#xA;å°†åŸæ•°æ®åˆ’åˆ†æˆé•¿æœŸç‰¹å¾ã€å­£èŠ‚æ€§ç‰¹å¾å’Œæ®‹å·®ç‰¹å¾ï¼Œä¸€ä¸ªè¾“å…¥æ•°æ®ç”±å¯¹åº”çš„ä¸‰ä¸ªç‰¹å¾å€¼ç›¸åŠ  é•¿æœŸç‰¹å¾ï¼ˆTrendï¼‰ $$ X_T \in \mathbb{R}^{n \times L}=\frac {1} {2k+1} \sum^k_{j=-k}(X_{i+j}) $$&#xA;å­£èŠ‚æ€§ç‰¹å¾ï¼ˆSeasonalï¼Œé‡‡ç”¨å±€éƒ¨åŠ æƒç§»åŠ¨å¹³å‡ï¼‰ $$ {Lowess\ Smoother}() $$&#xA;æ®‹å·®é¡¹ $$ X^i_R=X_i-X^i_T - X^i_S $$&#xA;æ­£åˆ™åŒ–æ–¹å¼ æ•°æ®æ„æˆå $$ {\hat x}{Tt}^i = \gamma_T(x{Tt}^i-\mathbb{E}[x_{Tt}^{i}]/\sqrt{Var[x_{Tt}^i]+\epsilon_T})+\beta_T $$&#xA;$\mathbb{E}[x_{Tt}^{i}]$æ˜¯å¹³å‡æ•° $Var[x_{Tt}^i]$æ˜¯æ ‡å‡†å·® $\gamma_T$å’Œ$\beta_T$æ˜¯å¯å­¦ä¹ çš„åç§»å‚æ•° åœ¨æ„æˆæ•°æ®ä¹‹åï¼Œæ¯ä¸ªç»„æˆéƒ¨åˆ†éƒ½è¿›è¡Œreverse instance normalizationï¼Œèƒ½å¤Ÿï¼ˆä¿ƒè¿›çŸ¥è¯†è½¬ç§»ï¼Œå¹¶æœ€å¤§ç¨‹åº¦å¾—å‡å°è½¬ç§»æŸå¤±ï¼Ÿï¼‰ã€‚ä½†ç›¸å¯¹åº”çš„ï¼Œåœ¨è¾“å‡ºå‰éœ€è¦é€†å‘æ ‡å‡†åŒ–å›æ¥ã€‚&#xA;é¢„æµ‹å $$ {\widehat Y}{*t}^i=\sqrt{Var[x{Tt}^i]+\epsilon_} \cdot {(\frac{Y_{t}^i-\beta_} {\gamma_})+\mathbb{E}[x_{Tt}^{i}]} $$</description>
    </item>
    <item>
      <title></title>
      <link>https://xcanton.github.io/notes/llm_notes/tuning/moe-lora/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/tuning/moe-lora/</guid>
      <description>MOE-Lora </description>
    </item>
    <item>
      <title></title>
      <link>https://xcanton.github.io/notes/llm_notes/tuning/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/notes/llm_notes/tuning/readme/</guid>
      <description>tuning </description>
    </item>
    <item>
      <title>Math Braintrainer</title>
      <link>https://xcanton.github.io/projects/math-brain-trainer/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/projects/math-brain-trainer/</guid>
      <description></description>
    </item>
    <item>
      <title>Surprise Surprise</title>
      <link>https://xcanton.github.io/articles/article/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://xcanton.github.io/articles/article/</guid>
      <description>Thank you for your support! Hello. If you like this template, I&amp;rsquo;d be happy to get a coffee donation :)</description>
    </item>
  </channel>
</rss>
